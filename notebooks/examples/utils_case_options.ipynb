{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the awk commands for parsing the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shutil import rmtree\n",
    "\n",
    "root_path = os.path.join(Path().resolve().parent.parent)\n",
    "package_path = os.path.join(root_path, \"hamageolib\")\n",
    "test_fixture_path = os.path.join(root_path, \"big_tests\", \"TwoDSubduction\", \"eba_cdpt_coh500_SA80.0_cd7.5_log_ss300.0\")\n",
    "result_path = os.path.join(Path().resolve(), \"results\")\n",
    "output_dir = os.path.join(result_path, \"case_options_output\")\n",
    "\n",
    "if str(package_path) not in sys.path:\n",
    "    sys.path.insert(0, str(package_path))\n",
    "\n",
    "if not os.path.isdir(result_path):\n",
    "    os.mkdir(result_path)\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "from utils.case_options import parse_log_file_for_time_info_to_pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Log File for Time Step and Wall Clock Information\n",
    "\n",
    "This block extracts and processes time step and wall clock data from an ASPECT log file.\n",
    "\n",
    "- Define the path to the log file: $ \\text{log\\_file\\_path} $ within the test fixture directory.\n",
    "- Define the output file path: $ \\text{output\\_path} $ for storing parsed results.\n",
    "- Use the function `parse_log_file_for_time_info_to_pd()` to extract time-related information into a pandas DataFrame.\n",
    "- The resulting pandas data structure contains a \"Corrected Wall Clock\" as output for wall clocks corrected for restarting the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = os.path.join(test_fixture_path, \"output\", \"log.txt\")\n",
    "output_path = os.path.join(result_path, \"parse_time_info_results\")\n",
    "\n",
    "time_dat = parse_log_file_for_time_info_to_pd(log_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CASE_OPTIONS class to load a case\n",
    "\n",
    "This class read the parameter files from aspect and related files from WorldBuilder.\n",
    "It stores the useful options for further analysis (uncomment the print options below to output).\n",
    "\n",
    "- case_dir (str): Directory path of the case.\n",
    "- output_dir (str): Directory path for output files.\n",
    "- visit_file (str): Path to the visit file for case visualization.\n",
    "- paraview_file (str): Path to the paraview file for case visualization.\n",
    "- img_dir (str): Directory path for image outputs.\n",
    "- idict (dict): Dictionary containing parsed parameters from the .prm file.\n",
    "- wb_dict (dict): Dictionary containing parsed parameters from the .wb file if it exists.\n",
    "- options (dict): Dictionary storing interpreted options for data output and model parameters.\n",
    "\n",
    "More specifically, some small files from ASPECT are already imported into handy format\n",
    "\n",
    "- statistic_df: a pandas object contains the data from the \"output/statistics\"\n",
    "- time_df: a pandas object with parsed data from the \"output/log.txt\" contains the time step, model time, and wallclock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.case_options import CASE_OPTIONS\n",
    "\n",
    "Case_Options = CASE_OPTIONS(test_fixture_path)\n",
    "\n",
    "# print(Case_Options.__dict__)\n",
    "# print(Case_Options.statistic_df)\n",
    "# print(Case_Options.time_df)\n",
    "# print(Case_Options.visualization_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a summary of the current case\n",
    "\n",
    "Note: first, the class needs to be interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Vtu step          Time  Time step number  Vtu snapshot  File found\n",
      "4           0  0.000000e+00                 0             4        True\n",
      "5           1  1.029984e+05                27             5        True\n",
      "6           2  2.024379e+05                46             6        True\n",
      "7           3  3.065885e+05                62             7        True\n",
      "8           4  4.017505e+05                75             8        True\n",
      "..        ...           ...               ...           ...         ...\n",
      "96         92  9.206712e+06               969            96        True\n",
      "97         93  9.300098e+06               980            97        True\n",
      "98         94  9.404288e+06               996            98        True\n",
      "99         95  9.501962e+06              1022            99        True\n",
      "100        96  9.600502e+06              1062           100        True\n",
      "\n",
      "[97 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "Case_Options.Interpret()\n",
    "Case_Options.SummaryCaseVtuStep()\n",
    "print(Case_Options.summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the CASE_SUMMARY class to generate summary of a project\n",
    "\n",
    "This code loops through **case directories** inside `\"big_tests/TwoDSubduction\"` and updates the case summary. \n",
    "\n",
    "#### **To locate a case:**\n",
    "- Each valid case directory **must contain a `.prm` file**.\n",
    "- The `.prm` file is identified using `find_case_files(item_path)`.\n",
    "- If no `.prm` file is found, the directory is skipped.\n",
    "- The summary stores essential metadata about cases.\n",
    "\n",
    "#### **Output:**\n",
    "- The script updates the df pandas object as an attribute of the class\n",
    "- For example, `Case_Summary.df[\"basename\"]` stores processed case names.\n",
    "- The `Case_Summary.df.columns` stores all the available names.\n",
    "- The `Case_Summary.df.attrs[\"units\"]` stores all the units.\n",
    "- The final output prints the collected case information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file /home/lochy/ASPECT_PROJECT/HaMaGeoLib/notebooks/examples/results/case_options_output/case_summary.csv\n",
      "                                          basename  name end time step  \\\n",
      "0                                       test_visit  None             5   \n",
      "1  eba_cdpt_coh500_SA80.0_OA40.0_cd100.0_cd7.5_gr9  None          3337   \n",
      "2                 EBA_CDPT_test_perplex_mixing_log  None          4040   \n",
      "3         eba_cdpt_coh500_SA80.0_cd7.5_log_ss300.0  None          1170   \n",
      "4                             test_get_snaps_steps  None           0.0   \n",
      "5                               test_visit_default  None             0   \n",
      "\n",
      "     end time  wall clock                                           abs path  \n",
      "0   1000000.0        21.6  /home/lochy/ASPECT_PROJECT/HaMaGeoLib/big_test...  \n",
      "1  60000000.0    653000.0  /home/lochy/ASPECT_PROJECT/HaMaGeoLib/big_test...  \n",
      "2  60000000.0    968000.0  /home/lochy/ASPECT_PROJECT/HaMaGeoLib/big_test...  \n",
      "3   9627890.0    526000.0  /home/lochy/ASPECT_PROJECT/HaMaGeoLib/big_test...  \n",
      "4         0.0         0.0  /home/lochy/ASPECT_PROJECT/HaMaGeoLib/big_test...  \n",
      "5         0.0        77.4  /home/lochy/ASPECT_PROJECT/HaMaGeoLib/big_test...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lochy/ASPECT_PROJECT/HaMaGeoLib/hamageolib/utils/case_options.py:414: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from utils.case_options import CASE_SUMMARY, find_case_files\n",
    "\n",
    "# Initiate the class\n",
    "Case_Summary = CASE_SUMMARY()\n",
    "\n",
    "# Assign a directory to loop over\n",
    "test_fixture_parent_dir = os.path.join(root_path, \"big_tests\", \"TwoDSubduction\")\n",
    "\n",
    "# find case directory and update summary\n",
    "for item in os.listdir(test_fixture_parent_dir):\n",
    "    item_path = os.path.join(test_fixture_parent_dir, item)\n",
    "\n",
    "    if os.path.isdir(item_path):\n",
    "        prm_file, _ = find_case_files(item_path)\n",
    "        \n",
    "        if prm_file is not None:\n",
    "            Case_Summary.update_single_case(item_path)\n",
    "\n",
    "file_path = os.path.join(output_dir, \"case_summary.csv\")\n",
    "Case_Summary.df.to_csv(file_path)\n",
    "print(\"Saved file %s\" % (file_path))\n",
    "\n",
    "# print(str(Case_Summary.df))\n",
    "# print(str(Case_Summary.df.attrs[\"units\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to a latex table\n",
    "\n",
    "- A `column_names` could be added to select the columns to export. Otherwise all columns are exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LaTeX table saved to /home/lochy/ASPECT_PROJECT/HaMaGeoLib/notebooks/examples/results/case_options_results.tex'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.case_options import df_to_latex\n",
    "\n",
    "output_path = os.path.join(result_path, \"case_options_results.tex\")\n",
    "\n",
    "columnn_names = [\"name\", \"end time step\", \"end time\"]\n",
    "\n",
    "df_to_latex(Case_Summary.df, output_path, columnn_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmgeolib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
