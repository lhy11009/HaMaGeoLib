{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the awk commands for parsing the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shutil import rmtree\n",
    "\n",
    "root_path = os.path.join(Path().resolve().parent.parent)\n",
    "package_path = os.path.join(root_path, \"hamageolib\")\n",
    "test_fixture_path = os.path.join(root_path, \"big_tests\", \"TwoDSubduction\", \"eba_cdpt_coh500_SA80.0_cd7.5_log_ss300.0\")\n",
    "result_path = os.path.join(Path().resolve(), \"results\")\n",
    "output_dir = os.path.join(result_path, \"case_options_output\")\n",
    "\n",
    "if str(package_path) not in sys.path:\n",
    "    sys.path.insert(0, str(package_path))\n",
    "\n",
    "if not os.path.isdir(result_path):\n",
    "    os.mkdir(result_path)\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "from utils.case_options import parse_log_file_for_time_info_to_pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Log File for Time Step and Wall Clock Information\n",
    "\n",
    "This block extracts and processes time step and wall clock data from an ASPECT log file.\n",
    "\n",
    "- Define the path to the log file: $ \\text{log\\_file\\_path} $ within the test fixture directory.\n",
    "- Define the output file path: $ \\text{output\\_path} $ for storing parsed results.\n",
    "- Use the function `parse_log_file_for_time_info_to_pd()` to extract time-related information into a pandas DataFrame.\n",
    "- The resulting pandas data structure contains a \"Corrected Wall Clock\" as output for wall clocks corrected for restarting the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = os.path.join(test_fixture_path, \"output\", \"log.txt\")\n",
    "output_path = os.path.join(result_path, \"parse_time_info_results\")\n",
    "\n",
    "time_dat = parse_log_file_for_time_info_to_pd(log_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CASE_OPTIONS class to load a case\n",
    "\n",
    "This class read the parameter files from aspect and related files from WorldBuilder.\n",
    "It stores the useful options for further analysis (uncomment the print options below to output).\n",
    "\n",
    "- case_dir (str): Directory path of the case.\n",
    "- output_dir (str): Directory path for output files.\n",
    "- visit_file (str): Path to the visit file for case visualization.\n",
    "- paraview_file (str): Path to the paraview file for case visualization.\n",
    "- img_dir (str): Directory path for image outputs.\n",
    "- idict (dict): Dictionary containing parsed parameters from the .prm file.\n",
    "- wb_dict (dict): Dictionary containing parsed parameters from the .wb file if it exists.\n",
    "- options (dict): Dictionary storing interpreted options for data output and model parameters.\n",
    "\n",
    "More specifically, some small files from ASPECT are already imported into handy format\n",
    "\n",
    "- statistic_df: a pandas object contains the data from the \"output/statistics\"\n",
    "- time_df: a pandas object with parsed data from the \"output/log.txt\" contains the time step, model time, and wallclock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.case_options import CASE_OPTIONS\n",
    "\n",
    "Case_Options = CASE_OPTIONS(test_fixture_path)\n",
    "\n",
    "# print(Case_Options.__dict__)\n",
    "# print(Case_Options.statistic_df)\n",
    "# print(Case_Options.time_df)\n",
    "# print(Case_Options.visualization_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a summary of the current case\n",
    "\n",
    "Note: first, the class needs to be interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case_Options.Interpret()\n",
    "Case_Options.SummaryCaseVtuStep()\n",
    "print(Case_Options.summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the CASE_SUMMARY class to generate summary of a project\n",
    "\n",
    "This code loops through **case directories** inside `\"big_tests/TwoDSubduction\"` and updates the case summary. \n",
    "\n",
    "#### **To locate a case:**\n",
    "- Each valid case directory **must contain a `.prm` file**.\n",
    "- The `.prm` file is identified using `find_case_files(item_path)`.\n",
    "- If no `.prm` file is found, the directory is skipped.\n",
    "- The summary stores essential metadata about cases.\n",
    "\n",
    "#### **Output:**\n",
    "- The script updates the df pandas object as an attribute of the class\n",
    "- For example, `Case_Summary.df[\"basename\"]` stores processed case names.\n",
    "- The `Case_Summary.df.columns` stores all the available names.\n",
    "- The `Case_Summary.df.attrs[\"units\"]` stores all the units.\n",
    "- The final output prints the collected case information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.case_options import CASE_SUMMARY, find_case_files\n",
    "\n",
    "# Initiate the class\n",
    "Case_Summary = CASE_SUMMARY()\n",
    "\n",
    "# Assign a directory to loop over\n",
    "test_fixture_parent_dir = os.path.join(root_path, \"big_tests\", \"TwoDSubduction\")\n",
    "\n",
    "# find case directory and update summary\n",
    "for item in os.listdir(test_fixture_parent_dir):\n",
    "    item_path = os.path.join(test_fixture_parent_dir, item)\n",
    "\n",
    "    if os.path.isdir(item_path):\n",
    "        prm_file, _ = find_case_files(item_path)\n",
    "        \n",
    "        if prm_file is not None:\n",
    "            Case_Summary.update_single_case(item_path)\n",
    "\n",
    "file_path = os.path.join(output_dir, \"case_summary.csv\")\n",
    "Case_Summary.df.to_csv(file_path)\n",
    "print(\"Saved file %s\" % (file_path))\n",
    "\n",
    "# print(str(Case_Summary.df))\n",
    "# print(str(Case_Summary.df.attrs[\"units\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to a latex table\n",
    "\n",
    "- A `column_names` could be added to select the columns to export. Otherwise all columns are exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.case_options import df_to_latex\n",
    "\n",
    "output_path = os.path.join(result_path, \"case_options_results.tex\")\n",
    "\n",
    "columnn_names = [\"name\", \"end time step\", \"end time\"]\n",
    "\n",
    "df_to_latex(Case_Summary.df, output_path, columnn_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmgeolib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
