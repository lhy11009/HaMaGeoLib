{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the environment of py-gplate\n",
    "import sys\n",
    "import gplately\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import cartopy.crs as ccrs\n",
    "from plate_model_manager import PlateModelManager\n",
    "\n",
    "# Include this pakage\n",
    "HaMaGeoLib_DIR = \"/home/lochy/ASPECT_PROJECT/HaMaGeoLib\"\n",
    "if os.path.abspath(HaMaGeoLib_DIR) not in sys.path:\n",
    "    sys.path.append(os.path.abspath(HaMaGeoLib_DIR))\n",
    "\n",
    "from hamageolib.research.haoyuan_3d_subduction.gplately_utilities import ResampleAllSubduction, mask_by_pids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow for Subduction Dataset from Plate Reconstruction\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "- Download the utilities file at (TODO: package this file)\n",
    "\n",
    "    https://github.com/lhy11009/Utilities/blob/master/python_scripts/py\n",
    "\n",
    "and put it into a directory called\n",
    "\n",
    "    \"utilities\"\n",
    "\n",
    "- Install the gplately package (TODO: include a conda file)\n",
    "\n",
    "- One must first use gplate and export a \"reconstruction\" file for the specific timestep. This file serves to look up names and ids of the subduction zones.\n",
    "\n",
    "#### General workflow\n",
    "\n",
    "This workflow defines the steps to query a subduction plate dataset based on plate reconstruction data.\n",
    "\n",
    "To get a good dataset for a time step:\n",
    "\n",
    "1. **Select the Automatic Workflow and get a global dataset**:\n",
    "   - Use the global dataset with the automatic workflow to initiate the data processing. This is a good reference dataset to refine points of respective trenches.\n",
    "\n",
    "\n",
    "2. **Look in the global dataset for a local subset or extract a Local dataset directly**:\n",
    "    - To directly extract a local dataset, look for a id for a trench. Put that in the workflow to extract a single trench.\n",
    "    - When we are satisfied, damp the data in to a finalized dataset.\n",
    "\n",
    "3. **Resample the subduction zones**\"\n",
    "    - To discretinize the subduction zones with the appropriate interval (e.g. 200 km), an option to rample them is included in the notebook.\n",
    "\n",
    "3. **Analyze the Finalzed Dataset** \n",
    "\n",
    "\n",
    "#### Some Notes\n",
    "\n",
    "##### Fixing Subducting Plate Age\n",
    "To accurately determine the age of the subducting plate, we must ensure that the points are correctly pinned to the subducting plate. This involves two workflows:\n",
    "\n",
    "**Automatic Workflow**: By calling the fill_NaNs value in the age raster. This will automatic fill-in the invalid vlaues\n",
    "\n",
    "    age_grid_raster.fill_NaNs(inplace=True)\n",
    "\n",
    "##### Trench pid and subduction pid\n",
    "\n",
    "The **trench PID** and **subduction PID** correspond to the IDs used in the plate reconstruction process (TODO: look carefully in their paper).\n",
    "\n",
    "- **Subduction PID**: The process ID (PID) for the subduction zone, as used in the reconstruction.\n",
    "- **Trench PID**: The process ID (PID) for the trench, as used in the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions\n",
    "\n",
    "- **mask_by_pids**: Generates a mask based on proximity to specified subducting and trench process IDs.\n",
    "- **haversine**: Calculates the great-circle distance between two geographical points on Earth using the Haversine formula.\n",
    "- **ReadFile**: Reads a file of plate reconstruction data to extract subduction zone details, including locations and IDs.\n",
    "- **LookupNameByPid**: Finds the name of a trench given its corresponding plate ID.\n",
    "- **get_one_subduction_by_trench_id**: Retrieves data for a specific subduction zone from a global dataset based on trench ID.\n",
    "- **plot_global_basics**: Plots basic global geological features on a map, including coastlines and geological age grids.\n",
    "- **resample_subduction**: Resamples subduction zone data along its arc length at specified intervals for simplified analysis.\n",
    "- **ResampleAllSubduction**: Resamples all subduction zones in a dataset using specified trench plate IDs.\n",
    "- **ResampleSubductionById**: Resamples data for a specific subduction zone based on its trench plate ID.\n",
    "- **FixTrenchAgeLocal**: Interpolates and fixes missing age values in subduction data using nearby points.\n",
    "- **FixTrenchAge**: Fixes invalid trench age values in subduction data using various age interpolation methods.\n",
    "- **MaskBySubductionTrenchIds**: Creates a mask for subduction data based on specified subducting and trench IDs or user-selected indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "At first run, run these tests by setting run_tests = True.\n",
    "Afterward, set this option to False to skip the test\n",
    "\n",
    "Below is a description of the tests\n",
    "\n",
    "- **test_name_lookup_read_file**: Verifies the `ReadFile` function's ability to correctly read and parse subduction zone data by checking the count of zones and plate ID occurrences.\n",
    "- **test_main_workflow**: Tests the main workflow by initializing plate reconstruction components, performing subduction zone tessellation, and verifying the resampled subduction data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests = True\n",
    "\n",
    "def test_name_lookup_read_file():\n",
    "    \"\"\"\n",
    "    Tests the `ReadFile` function to ensure it correctly reads and parses subduction zone data.\n",
    "    \n",
    "    Implementation:\n",
    "        - Loads the subduction name lookup file from a specified path.\n",
    "        - Calls the `ReadFile` function with the provided file path.\n",
    "        - Asserts that the number of subduction zones (`n_trench`) is as expected.\n",
    "        - Asserts that the plate ID 201 appears the expected number of times in the list of trench plate IDs.\n",
    "    \n",
    "    Assertions:\n",
    "        - The test checks if `n_trench` equals 52, confirming the total count of trenches.\n",
    "        - The test ensures that the plate ID 201 appears 7 times, validating the parsing of plate IDs.\n",
    "    \"\"\"\n",
    "    subduction_name_lookup_file = os.path.join(HaMaGeoLib_DIR, \"dtemp\", \"gplate_export_test0\", \n",
    "                                               \"Muller_etal_2019_PlateBoundaries_no_topologies\", \n",
    "                                               \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "    outputs = ReadFile(subduction_name_lookup_file)\n",
    "    assert(outputs['n_trench'] == 52)\n",
    "    assert(outputs[\"trench_pids\"].count(201) == 7)\n",
    "\n",
    "\n",
    "\n",
    "# if run_tests:\n",
    "    # test_name_lookup_read_file()\n",
    "    # test_main_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Begin by setting a **reconstruction time**, which `gplately` will use to reconstruct geological features at a specific moment in geological history. Additionally, define an **anchor plate ID**â€”this serves as a reference plate, stabilizing the reconstruction to this particular plate (TODO: research further for detailed impact).\n",
    "\n",
    "You'll also need a **subduction name lookup file** that links each subduction zone's name with its corresponding ID for each reconstruction time. Note that this file is not provided by `gplately` and must be exported separately from GPlates. (TODO: consider making this file optional in the workflow to enhance flexibility.)\n",
    "\n",
    "#### Parameters\n",
    "* **reconstruction_time** - The specific geological time for the reconstruction.\n",
    "* **subduction_name_lookup_file** - Exported file from GPlates containing subduction zone names and their IDs, as `gplately` does not retain this information.\n",
    "* **anchor_plate_id** - The plate used as a reference for stabilizing the reconstruction.\n",
    "* **all_columns** - A list of column names used in the GPlates data to ensure consistent data mapping.\n",
    "* **plate_model** - Specifies the reconstruction model to use; refer to the `gplately` tutorial for additional guidance on available models and usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamageolib.research.haoyuan_3d_subduction.gplately_utilities import ReadFile\n",
    "\n",
    "# assign a reconstruction time\n",
    "reconstruction_time=0 # time of reconstruction, must be integar\n",
    "\n",
    "# enter the directory of the plate reconstruction files\n",
    "dir_re = os.path.join(HaMaGeoLib_DIR, \"dtemp/gplate_export_test0/Muller_etal_2019_PlateBoundaries_no_topologies\")\n",
    "subduction_name_lookup_file = os.path.join(HaMaGeoLib_DIR, \"dtemp\", \"gplate_export_test0\", \"Muller_etal_2019_PlateBoundaries_no_topologies\", \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "# fact checks\n",
    "assert(type(reconstruction_time) == int)\n",
    "assert(os.path.isdir(dir_re))\n",
    "\n",
    "# Initialize the anchor plate ID for the reconstruction model\n",
    "anchor_plate_id = 0\n",
    "\n",
    "# Define the columns used in the subduction data DataFrame\n",
    "all_columns = ['lon', 'lat', 'conv_rate', 'conv_angle', 'trench_velocity', \n",
    "                          'trench_velocity_angle', 'arc_length', 'trench_azimuth_angle', \n",
    "                          'subducting_pid', 'trench_pid']\n",
    "\n",
    "# Create an instance of the PlateModelManager to manage plate models\n",
    "pm_manager = PlateModelManager()\n",
    "\n",
    "# Load the \"Muller2019\" plate model from the specified data directory\n",
    "plate_model = pm_manager.get_model(\"Muller2019\", data_dir=\"plate-model-repo\")\n",
    "\n",
    "# Set up the PlateReconstruction model using the loaded plate model data\n",
    "# This includes rotation models, topologies, and static polygons, with the specified anchor plate ID\n",
    "model = gplately.PlateReconstruction(\n",
    "    plate_model.get_rotation_model(), \n",
    "    plate_model.get_topologies(), \n",
    "    plate_model.get_static_polygons(),\n",
    "    anchor_plate_id=anchor_plate_id\n",
    ")\n",
    "\n",
    "# Initialize the plotting object for visualizing topologies\n",
    "# The layers used for plotting include coastlines, continental polygons, and COBs (Continental Ocean Boundaries)\n",
    "gplot = gplately.plot.PlotTopologies(\n",
    "    model, \n",
    "    plate_model.get_layer('Coastlines'), \n",
    "    plate_model.get_layer('ContinentalPolygons'), \n",
    "    plate_model.get_layer('COBs')\n",
    ")\n",
    "\n",
    "# get the reconstruction of subduction zones\n",
    "subduction_data = model.tessellate_subduction_zones(reconstruction_time, \n",
    "                                                   # tessellation_threshold_radians=0.01, \n",
    "                                                    anchor_plate_id=anchor_plate_id,\n",
    "                                                    ignore_warnings=True)\n",
    "# get all the trench ids\n",
    "temp = [row[9] for row in subduction_data]\n",
    "trench_pids = sorted(set(temp))\n",
    "\n",
    "# Initialize the age grid raster, which will be used for age-related computations\n",
    "age_grid_raster = gplately.Raster(\n",
    "                                data=plate_model.get_raster(\"AgeGrids\",reconstruction_time),\n",
    "                                plate_reconstruction=model,\n",
    "                                extent=[-180, 180, -90, 90]\n",
    "                                )\n",
    "# fill Nan values, it seems to not cause any issue in interpolating the ages.\n",
    "# otherwise, there are many points where the trench point are not covered in the Raster.\n",
    "# Thus, it seems these points are just on the boundary where some other value could be filled.\n",
    "age_grid_raster.fill_NaNs(inplace=True)\n",
    "\n",
    "# parse the name lookup file\n",
    "name_lookups = ReadFile(subduction_name_lookup_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some General Utilities\n",
    "\n",
    "#### Search a Single Subduction Zone\n",
    "\n",
    "##### Search with a key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(name_lookups[\"trench_names\"])\n",
    "\n",
    "# keyword = \"ryu\"\n",
    "\n",
    "# matching_indices = [i for i, name in enumerate(name_lookups[\"trench_names\"]) if keyword.lower() in name.lower()]\n",
    "# for index in matching_indices:\n",
    "#     print(index)\n",
    "#     print(\"name: \", name_lookups[\"trench_names\"][index])\n",
    "#     print(\"id: \", name_lookups[\"trench_pids\"][index])\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search with a trench id\n",
    "\n",
    "The following block creates a global map displaying geological features and highlights a specific subduction\n",
    "In order to do this, it first looks up in a global dataset. And then plot the one subduction zone in a global map\n",
    "\n",
    "Below are some examples of the trench ids\n",
    "\n",
    "* 12001, ,CAS\n",
    "* 686, Indonesian bndy w AUS-mg, ANDA-SUM\n",
    "* 736, Java SZ\n",
    "* 651, Flores Banda SZ, JAVA\n",
    "* 669, North Sulawesi Subduction, SULA\n",
    "* 612, Luzon subduction, LUZ\n",
    "* 678, Philippine trench, PHIL\n",
    "* 648, Okinawa Trough (Ryuku) from EarthByte cob MG 4-20-07\n",
    "* 659, Izu Bonin Trench\n",
    "* 699, Marianas Trench-NUVEL\n",
    "* 111, Aleutian and Bering Sea Masking Polygon\n",
    "* 406, Kamchatka SZ from EarthByte COB at 0Ma -- MG 4/20/07\n",
    "* 413, Shirshov Ridge Subduction (not included), part of 901 (subduction id)\n",
    "* 2000, Central American subduction from 135 Ma\n",
    "* 201, South America trench taken from a combination of RUM and COB file\n",
    "* 2031, Caribbean/farallon subduction iniating at 85 Ma\n",
    "* 2011, Caribbean subduction\n",
    "* 815, Sandwich Trench\n",
    "* 821, Tonga-Kermadec-MG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: the current plot functions result in a redundant grid around the actual plot\n",
    "# # Define the trench plate ID\n",
    "# trench_pid = 111\n",
    "# # Look up the name of the subduction zone using the trench ID\n",
    "# _name = LookupNameByPid(name_lookups[\"trench_pids\"], name_lookups[\"trench_names\"], trench_pid)\n",
    "# print(_name)\n",
    "\n",
    "# # Get the data for the specified subduction zone using the trench ID\n",
    "# # The data is returned as a pandas DataFrame\n",
    "# one_subduction_data = get_one_subduction_by_trench_id(subduction_data, trench_pid, all_columns)\n",
    "\n",
    "# # Create a new figure for plotting\n",
    "# fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "# plt.title(f'{reconstruction_time} Ma')  # Set the title to the reconstruction time\n",
    "\n",
    "# # Set up the axes with a Mollweide projection, centered at longitude 180\n",
    "# ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude=180))\n",
    "\n",
    "# # Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "# ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "# # Plot the subduction zone data as red scatter points\n",
    "# # Use the PlateCarree projection to correctly position the points on the map\n",
    "# im_sub = ax.scatter(one_subduction_data.lon, one_subduction_data.lat, \n",
    "#                     marker=\".\", s=3, c='r', transform=ccrs.PlateCarree())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow to extract a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Methodology\n",
    "\n",
    "In this section, we define the methodology for working with the subduction dataset.\n",
    "\n",
    "1. **Loading Data**: \n",
    "   - You can choose to either load an existing CSV file containing previously saved data or start the process from scratch.\n",
    "\n",
    "2. **Sampling Trenches**:\n",
    "   - You can choose between two sampling methods:\n",
    "     - **Sample all trenches at once**: This option allows you to sample all trenches in a single operation.\n",
    "     - **Sample a specific trench**: Alternatively, you can focus on sampling a single trench for more granular analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample by a give arc length edge and resample section\n",
    "# arc_length_edge = 0.0; arc_length_resample_section = 2.0  # by degree\n",
    "arc_length_edge = 2.0; arc_length_resample_section = 2.0  # by degree\n",
    "\n",
    "# use_recorded_file = True; resample_all = True; trench_pid = None # use this option to start from a recorded file\n",
    "use_recorded_file = False; resample_all = True; trench_pid = None # use this option to start from scratch and sample all subductions\n",
    "# use_recorded_file = False; resample_all = False; trench_pid = 2000 # use this option to start from scratch and sample one specific trench\n",
    "\n",
    "# resample the subduction zones\n",
    "\n",
    "## archived files\n",
    "# recorded_file = os.path.join(ASPECT_LAB_DIR, \"files\", \"ThDSubduction\", \"gplate_json_files\", \"subduction_resampled_t_%.2e.csv\" \\\n",
    "#                         % (reconstruction_time))\n",
    "recorded_file = os.path.join(HaMaGeoLib_DIR, \"files\", \"ThDSubduction\", \"gplate_json_files\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f_11112024.csv\" \\\n",
    "                             % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "\n",
    "## temp file\n",
    "# recorded_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.csv\" \\\n",
    "#                             % (reconstruction_time, arc_length_edge, arc_length_resample_section))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract from the dataset\n",
    "\n",
    "The following code block resamples the following dataframe:\n",
    "\n",
    "- **subduction_data_resampled**: DataFrame containing resampled subduction data, either read from a recorded file or generated by resampling all subduction zones or a specific trench ID.\n",
    "\n",
    "This contains some extra columns:\n",
    "\n",
    "- **lon_fix**: Column added to `subduction_data_resampled` to store corrected longitude values for visual or analytical adjustments, initialized with NaN values.\n",
    "- **lat_fix**: Column added to `subduction_data_resampled` to store corrected latitude values, initialized with NaN values.\n",
    "- **fix_age_polarity**: Column indicating the polarity of the age correction, helping to adjust for geological positioning, initialized with NaN values.\n",
    "- **marker**: Column used to specify marker types for visualization, initialized with NaN values.\n",
    "- **marker_fill**: Column defining the fill style of markers (e.g., 'none'), used in plotting or visualization tasks, initialized to \"none\" for all entries.\n",
    "- **color**: Column for specifying color information for each data point, primarily for visual representation, initialized with NaN values.\n",
    "- **age**: Column storing the interpolated ages for each subduction point, derived from `age_grid_raster` based on the latitude and longitude values in `subduction_data_resampled`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_recorded_file:\n",
    "    # If using a recorded file, ensure the file exists and load subduction data from it.\n",
    "    print(\"use recorded file: \", recorded_file)\n",
    "    assert(os.path.isfile(recorded_file))\n",
    "    subduction_data_resampled = pd.read_csv(recorded_file)\n",
    "else:\n",
    "    # Resample subduction data based on the specified option.\n",
    "    if resample_all:\n",
    "        # Resample all subduction zones if `resample_all` is set to True.\n",
    "        subduction_data_resampled = ResampleAllSubduction(\n",
    "            subduction_data, trench_pids, arc_length_edge, arc_length_resample_section, all_columns\n",
    "        )\n",
    "    else:\n",
    "        # Resample a specific subduction zone by trench ID if `resample_all` is False.\n",
    "        subduction_data_resampled, _ = ResampleSubductionById(\n",
    "            subduction_data, trench_pid, arc_length_edge, arc_length_resample_section, all_columns\n",
    "        )\n",
    "    \n",
    "    # Initialize new columns in the DataFrame for age correction and visualization attributes.\n",
    "    subduction_data_resampled['lon_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['lat_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['fix_age_polarity'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['marker'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['marker_fill'] = ['none' for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['color'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "\n",
    "    subduction_data_resampled['age']  = age_grid_raster.interpolate(subduction_data_resampled.lon, subduction_data_resampled.lat, method=\"nearest\")\n",
    "\n",
    "    # Output the number of rows for debugging purposes.\n",
    "    print(\"Total resampled points: \", len(subduction_data_resampled))\n",
    "\n",
    "    # Correct invalid or missing age values in the resampled data.\n",
    "    # DEPRICATED: with the age_grid_raster.fill_NaNs, the following function is not needed anymore.\n",
    "    # The following two lines are kept for test purposes\n",
    "\n",
    "    # subduction_data_resampled['age']  = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    # FixTrenchAge(subduction_data_resampled, age_grid_raster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for invalid age values\n",
    "\n",
    "Just double check that we get the correct age values everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the invalid indexes\n",
    "invalid_indexes = []\n",
    "for i in range(len(subduction_data_resampled['age'])):\n",
    "    if np.isnan(subduction_data_resampled['age'][i]):\n",
    "        invalid_indexes.append(i)\n",
    "\n",
    "print(\"len(ages): \")\n",
    "print(len(subduction_data_resampled['age']))\n",
    "print(\"ages: \")\n",
    "print(subduction_data_resampled['age'])\n",
    "print(\"invalid_indexes: \")\n",
    "print(invalid_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix the invalid values\n",
    "\n",
    "First, map out the invalid ages from the outputs from the previous section.\n",
    "\n",
    "Use the index, theta and direction to create a new sampling point interact with the raster data\n",
    "* i_fs: the index\n",
    "* theta_fs: theta of the direction. 0 is north and 180 is south.\n",
    "* d_fs: distance along the direction\n",
    "\n",
    "The next section is for plotting the current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_fs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "# theta_fs = [180.0, 180.0, 180.0, 180.0, 180.0, 180.0, 180.0, 210.0, 210.0, 150.0, 180.0, 180.0]\n",
    "# d_fs = [100e3, 100e3, 100e3, 100e3, 100e3, 200e3, 400e3, 300e3, 200e3, 300e3, 200e3, 1000e3]\n",
    "# for ii in range(len(i_fs)):\n",
    "#     i_f = i_fs[ii]\n",
    "#     theta_f = theta_fs[ii]\n",
    "#     d_f = d_fs[ii]\n",
    "#     subduction_data_resampled_local = None\n",
    "#     subduction_data_resampled_local = pd.DataFrame([subduction_data_resampled.iloc[i_f]])\n",
    "#     subduction_data_resampled_local.lon, subduction_data_resampled_local.lat = \\\n",
    "#         map_point_by_distance(subduction_data_resampled.iloc[i_f].lon, subduction_data_resampled.iloc[i_f].lat, theta_f, d_f)\n",
    "#     new_age = GClass.InterpolateAgeGrid(subduction_data_resampled_local)\n",
    "#     print(i_f, \": new age - \", new_age)\n",
    "#     subduction_data_resampled.loc[i_f, 'age'] = new_age\n",
    "#     if new_age is not np.nan:\n",
    "#         new_lon = subduction_data_resampled_local.lon.values[0]\n",
    "#         new_lat = subduction_data_resampled_local.lat.values[0]\n",
    "#         subduction_data_resampled.loc[i_f, 'lon_fix'] = new_lon\n",
    "#         subduction_data_resampled.loc[i_f, 'lat_fix'] = new_lat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix the dataset of Ryuku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ryuku_dataset = pd.DataFrame(np.nan, index=range(5), columns=subduction_data_resampled.columns)\n",
    "\n",
    "# ryuku_dataset['lat'] = [23.4, 24.2, 25.7, 27.5, 29.8]\n",
    "# ryuku_dataset['lon'] = [124.0, 127.0, 129.0, 130.5, 132.0]\n",
    "# ryuku_dataset['age'] = [35.0, 38.0, 48.0, 50.0, 50.0]\n",
    "# ryuku_dataset['trench_velocity']= [3.0, 0.9, 1.2, 0.7, 0.9]\n",
    "\n",
    "# subduction_data_resampled = pd.concat([subduction_data_resampled, ryuku_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add a vector to determined whether the slab is deep enough\n",
    "\n",
    "This feature is then used when calculating the distance of one slab to another. If the slab is tagged with \"deep slab\", then it will be selected as a potential barriar in the mantle and used in the calculation of the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_deep_slab = False\n",
    "\n",
    "if not use_recorded_file:    \n",
    "    # for the first time, initiate the column in the data\n",
    "    subduction_data_resampled['deep_slab'] = [1.0 for i in range(len(subduction_data_resampled))]\n",
    "\n",
    "if add_deep_slab:\n",
    "    \n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = None; assign_zero_trench_pid = None; exclude_fractional_pids=False # by indexed\n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = None; assign_zero_trench_pid = None;  exclude_fractional_pids=True# by indexed\n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = 645.0; assign_zero_trench_pid = 669.0; exclude_fractional_pids=False # by pids\n",
    "    \n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = 801.0; assign_zero_trench_pid = 827.0; exclude_fractional_pids=False # experiment to remove north Fiji\n",
    "\n",
    "    if len(assign_zero_indexes) > 0:\n",
    "        # by directly assign by indexes\n",
    "        mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "        for i in assign_zero_indexes:\n",
    "            mask[i] = 1\n",
    "    elif assign_zero_subducting_pid is not None:\n",
    "        # by subducting_pid and trench_pid\n",
    "        mask = mask_by_pids(subduction_data_resampled, assign_zero_subducting_pid, assign_zero_trench_pid)\n",
    "    elif exclude_fractional_pids:\n",
    "        # by excluding the fractional pid numbers\n",
    "        mask = (abs(subduction_data_resampled[\"subducting_pid\"] - np.round(subduction_data_resampled[\"subducting_pid\"])) > 0.1)\n",
    "    else:\n",
    "        mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "\n",
    "    subduction_data_resampled.loc[mask, \"deep_slab\"] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the dataset\n",
    "\n",
    "##### View the whole dataset or the seleted points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic plots\n",
    "from hamageolib.research.haoyuan_3d_subduction.gplately_utilities import LookupNameByPid, plot_global_basics\n",
    "\n",
    "plot_whole_dataset = True\n",
    "# plot the reconstructed zone\n",
    "# i_p = None; subducting_pid_p = None; trench_pid_p = None; # plot all\n",
    "# i_p = 76; subducting_pid_p = None; trench_pid_p = None; # plot one point\n",
    "i_p = None; subducting_pid_p = 608.0; trench_pid_p = 737.0; # plot one subduction zone by pid lookup\n",
    "\n",
    "if plot_whole_dataset:\n",
    "\n",
    "    if i_p is not None:\n",
    "        mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "        mask[i_p] = 1\n",
    "    elif subducting_pid_p is not None:\n",
    "        mask = mask_by_pids(subduction_data_resampled, subducting_pid_p, trench_pid_p)\n",
    "        # print the name of subduction zone\n",
    "        _name = LookupNameByPid(name_lookups[\"trench_pids\"], name_lookups[\"trench_names\"],\\\n",
    "            int(np.round(trench_pid_p)))\n",
    "        print(_name)\n",
    "    else:\n",
    "        mask = np.ones(len(subduction_data_resampled), dtype=bool)\n",
    "            \n",
    "\n",
    "    fig = plt.figure(figsize=(10,6), dpi=100)\n",
    "    ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude = 180))\n",
    "\n",
    "    # Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "    ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "    # debug\n",
    "    # print(subduction_data_resampled['age'])\n",
    "\n",
    "    # plot all the fixed ages\n",
    "    # mask = (~subduction_data_resampled['age'].isna())\n",
    "    \n",
    "    # plot all points\n",
    "    ax.scatter(subduction_data_resampled[mask].lon, subduction_data_resampled[mask].lat, marker=\".\", s=60, c='r', transform=ccrs.PlateCarree())\n",
    "    # ax.scatter(subduction_data_resampled[~mask].lon, subduction_data_resampled[~mask].lat, marker=\".\", s=60, c='y', transform=ccrs.PlateCarree())\n",
    "    ax.scatter(subduction_data_resampled[mask].lon_fix, subduction_data_resampled[mask].lat_fix, marker=\".\", s=30, c='c', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # write outputs\n",
    "    # fileout = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.pdf\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # fileout1 = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.png\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # if not (os.path.isdir(os.path.dirname(fileout))):\n",
    "    #     os.mkdir(os.path.dirname(fileout))\n",
    "    # fig.savefig(fileout)\n",
    "    # fig.savefig(fileout1)\n",
    "    # print(\"Save figure: %s\" % fileout)\n",
    "    # print(\"Save figure: %s\" % fileout1)\n",
    "\n",
    "else:\n",
    "    print(\"Skip plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select and plot the points based on nearness to a query point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamageolib.research.haoyuan_3d_subduction.gplately_utilities import haversine\n",
    "\n",
    "plot_near_points = False\n",
    "# plot the reconstructed zone\n",
    "# query longitude, latitude and distance\n",
    "lat_q = -14.0\n",
    "lon_q = 165.0\n",
    "dist_q = 500 # km\n",
    "\n",
    "n_near_points = 0\n",
    "point_indexes = []\n",
    "if plot_near_points:\n",
    "\n",
    "    mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "    for i in range(len(subduction_data_resampled)):\n",
    "        dist = haversine(lat_q, lon_q, subduction_data_resampled.lat[i], subduction_data_resampled.lon[i], radius=6371)\n",
    "        if dist < dist_q:\n",
    "            mask[i] = 1\n",
    "            n_near_points += 1\n",
    "            point_indexes.append(i)\n",
    "\n",
    "    # print the near points, subducting and trench pid, as well as trench name\n",
    "    subduction_data_resampled_masked = subduction_data_resampled[mask]\n",
    "    for i in range(n_near_points):\n",
    "        _name = LookupNameByPid(name_lookups[\"trench_pids\"], name_lookups[\"trench_names\"],\\\n",
    "            int(np.round(subduction_data_resampled_masked.trench_pid[point_indexes[i]])))\n",
    "        print(\"point indexes\\t\", \"lat\\t\\t\\t\" ,\"lon\\t\\t\\t\", \"subducting_pid\\t\", \"trench_pid\\t\", \"name\\t\")\n",
    "        print(point_indexes[i], \"\\t\\t\", subduction_data_resampled_masked.lat[point_indexes[i]], \"\\t\",\\\n",
    "              subduction_data_resampled_masked.lon[point_indexes[i]], \"\\t\",\\\n",
    "              subduction_data_resampled_masked.subducting_pid[point_indexes[i]],\"\\t\\t\",\\\n",
    "              subduction_data_resampled_masked.trench_pid[point_indexes[i]],\"\\t\\t\", _name,\"\\t\")\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6), dpi=100)\n",
    "    ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude = 180))\n",
    "\n",
    "    # Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "    ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "    # # debug\n",
    "    # print(subduction_data_resampled['age'])\n",
    "\n",
    "    # # plot all the fixed ages\n",
    "    # mask = (~subduction_data_resampled['age'].isna())\n",
    "    \n",
    "    # plot all points\n",
    "    ax.scatter(lon_q, lat_q, marker=\".\", s=200, c='purple', transform=ccrs.PlateCarree(), label=\"Query point\")\n",
    "    ax.scatter(subduction_data_resampled[mask].lon, subduction_data_resampled[mask].lat, marker=\".\", s=60, c='r', transform=ccrs.PlateCarree(), label=\"Near points\")\n",
    "    # ax.scatter(subduction_data_resampled[~mask].lon, subduction_data_resampled[~mask].lat, marker=\".\", s=60, c='y', transform=ccrs.PlateCarree())\n",
    "    # ax.scatter(subduction_data_resampled[mask].lon_fix, subduction_data_resampled[mask].lat_fix, marker=\".\", s=30, c='c', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # write outputs\n",
    "    # fileout = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.pdf\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # fileout1 = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.png\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # if not (os.path.isdir(os.path.dirname(fileout))):\n",
    "    #     os.mkdir(os.path.dirname(fileout))\n",
    "    # fig.savefig(fileout)\n",
    "    # fig.savefig(fileout1)\n",
    "    # print(\"Save figure: %s\" % fileout)\n",
    "    # print(\"Save figure: %s\" % fileout1)\n",
    "\n",
    "else:\n",
    "    print(\"Skip plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_file = False\n",
    "# export the file to a temp file\n",
    "if resample_all:\n",
    "    temp_file = os.path.join(HaMaGeoLib_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.csv\" \\\n",
    "                            % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "else:\n",
    "    temp_file = os.path.join(HaMaGeoLib_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_pid_%d_edge_%.2f_section_%.2f.csv\" \\\n",
    "                         % (reconstruction_time, int(trench_pid), arc_length_edge, arc_length_resample_section))\n",
    "\n",
    "# don't mess up the existing files\n",
    "if record_file:\n",
    "    if use_recorded_file:\n",
    "        # in case we read from an existing file, omit the index when output\n",
    "        subduction_data_resampled.to_csv(temp_file, index=False)\n",
    "    else:\n",
    "        subduction_data_resampled.to_csv(temp_file)\n",
    "\n",
    "    print(\"Data saved to %s\" % temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze extracted data set\n",
    "\n",
    "We plot:\n",
    "\n",
    "* age vs convergence / trench retreat rate of trenches below.\n",
    "* trench distance vs convergence / trench retreat rate of trenches below.\n",
    "\n",
    "#### Some notes\n",
    "\n",
    "##### On the Proccedure\n",
    "\n",
    "- Filtering short slab using the \"deep_slab\" vector would vary the result quite a bit.\n",
    "\n",
    "##### On specific subduction zones\n",
    "\n",
    "* TON-KERM: by calculating the distance, points are matched to the other side of north Fiji Basin, resulting in small distance value.\n",
    "* Ryuku: data is missing in the current dataset, fixing using values form Table 1 in Lallemend et al., 2005\n",
    "* JAVA: One single point has an advance motion.\n",
    "* LUZ: gplately data suggest advance of 2.5, while L05 shows fast retreat motion. When computing the distance, the LUZ will be matched to the east LUZ and result in very small values\n",
    "* KuKam: data shows retreat motion, while L05 shows advance motion\n",
    "* PER-NCHI-JUAN-SCHI (subduction id 911): These seem to be all in the subduction id 901. Beyond a trench id of 201, the trench id varies from 201010 to 20101?. Up north, the component is marked with trench id 2031 (Caribbean/farallon subduction iniating at 85 Ma).\n",
    "* South to SCHI: This is represented by the subduction id 801, but the trench id 201 continues from the previous 901 subduction.\n",
    "* ANT: retreat motion in this dataset, rather than the advance motion in L05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a distance to an adjacent subduction zone\n",
    "\n",
    "**Find Nearest Deep Subduction Points**:\n",
    "   - For each subduction point, iterates over all other points in the dataset.\n",
    "   - Filters out shallow subduction points and those within the same subduction zone.\n",
    "   - Calculates the distance and bearing angle to each other point, checking if it falls within the specified angle tolerance (`theta_diff`).\n",
    "   - Updates the minimum distance if a closer deep subduction point is found within the angle tolerance (initialized as the length of the Earth's equator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Geod\n",
    "from hamageolib.research.haoyuan_2d_subduction.legacy_utilities import map_point_by_distance, shortest_path_between_two_points, calculate_bearing\n",
    "\n",
    "# Define Earth's radius in meters\n",
    "Ro = 6371e3\n",
    "\n",
    "# Initialize the Geod object with the WGS84 ellipsoid model, \n",
    "# which provides accurate geodesic calculations on Earth's surface\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Initialize arrays to store distances, indexes, and markers for subduction data points\n",
    "subduction_distances = np.zeros(len(subduction_data_resampled))  # Stores the minimum distance for each subduction point\n",
    "subduction_indexes = np.array(range(len(subduction_data_resampled)))  # Indexes for each point in the dataset\n",
    "subduction_min_distance_indexes = np.full(len(subduction_data_resampled), -1, dtype=int)  # Indexes of the closest deep subduction point\n",
    "subduction_maker_lons = []  # Stores longitudes of marker points\n",
    "subduction_maker_lats = []  # Stores latitudes of marker points\n",
    "\n",
    "# Define the default comparison distance as Earth's circumference\n",
    "# and set parameters for marker distance and angle tolerance\n",
    "d_m = 1000e3  # Distance of the marker point in meters\n",
    "theta_diff = 45.0  # Tolerance angle in degrees for direction comparison\n",
    "\n",
    "# Iterate over each subduction point in the resampled data to compute closest distances\n",
    "for i_1 in range(len(subduction_data_resampled)):\n",
    "    subducting_pid_1 = int(subduction_data_resampled.subducting_pid[i_1])\n",
    "    lat1, lon1 = subduction_data_resampled.lat[i_1], subduction_data_resampled.lon[i_1]\n",
    "\n",
    "    # Place a marker point at a distance `d_m` along the trench normal angle (`theta`)\n",
    "    theta = subduction_data_resampled.trench_azimuth_angle[i_1]\n",
    "    theta = (theta + 360) % 360  # Normalize angle `theta` to the range [0, 360)\n",
    "    lon_m, lat_m = map_point_by_distance(lon1, lat1, theta, d_m)\n",
    "\n",
    "    # Get the path coordinates between the original point and marker point\n",
    "    path_lon_m, path_lat_m = shortest_path_between_two_points([lon1, lat1], [lon_m, lat_m], geod.npts, 100)\n",
    "    subduction_maker_lons.append(path_lon_m)\n",
    "    subduction_maker_lats.append(path_lat_m)\n",
    "\n",
    "    # Set initial comparison distance to Earth's circumference\n",
    "    distance = 2 * np.pi * Ro\n",
    "\n",
    "    # Find the closest deep subduction point with matching angle tolerance\n",
    "    for j_2 in range(len(subduction_data_resampled)):\n",
    "        subducting_pid_2 = int(subduction_data_resampled.subducting_pid[j_2])\n",
    "\n",
    "        # Skip shallow slabs (< 410 km depth)\n",
    "        if not subduction_data_resampled.deep_slab[j_2]:\n",
    "            continue\n",
    "        \n",
    "        # Skip points on the same subduction zone (including itself)\n",
    "        if subducting_pid_1 == subducting_pid_2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the distance and bearing to the other point\n",
    "        lat2, lon2 = subduction_data_resampled.lat[j_2], subduction_data_resampled.lon[j_2]\n",
    "        theta_1 = calculate_bearing(lon1, lat1, lon2, lat2)  # Bearing angle to the other point\n",
    "        distance_between_points = haversine(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "        # Update distance if closer point is found within the angle tolerance\n",
    "        if distance_between_points < distance and abs(theta_1 - theta) < theta_diff:\n",
    "            distance = distance_between_points\n",
    "            subduction_min_distance_indexes[i_1] = j_2\n",
    "    \n",
    "    # Store the computed minimum distance for the current point\n",
    "    subduction_distances[i_1] = distance\n",
    "\n",
    "# Add the computed distances to the DataFrame as a new column\n",
    "subduction_data_resampled[\"near_distance\"] = subduction_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the matching points of minimum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamageolib.research.haoyuan_3d_subduction.gplately_utilities import MaskBySubductionTrenchIds\n",
    "\n",
    "# Define plotting parameters to specify which subduction zone or point to display\n",
    "i_p = None; subducting_pid = None; trench_pid=None # Uncomment to plot one subduction zone\n",
    "# i_p = 43; subducting_pid = None; trench_pid=None # Uncomment to plot one point\n",
    "# i_p = None; subducting_pid = 801; trench_pid = None  # Plot specific subduction zone with subducting plate ID\n",
    "\n",
    "# Initialize the figure and axis with a Mollweide projection for global plotting\n",
    "fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude=180))\n",
    "\n",
    "# Plot the global coastline, age grid, and other basic features on the map\n",
    "plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "# Create masks to filter subduction points based on specified parameters\n",
    "mask1 = MaskBySubductionTrenchIds(subduction_data_resampled, subducting_pid, trench_pid, i_p)  # Filter by IDs\n",
    "mask2 = (subduction_min_distance_indexes >= 0)  # Filter points with valid trench distances\n",
    "\n",
    "# Combine masks to get points with and without valid trench distances\n",
    "mask = mask1 & mask2  # Valid points with trench distance\n",
    "mask_in = mask1 & (~mask2)  # Invalid points without trench distance\n",
    "\n",
    "# Separate valid and invalid indexes for plotting\n",
    "min_indexes_invalid = subduction_min_distance_indexes[mask_in]\n",
    "min_indexes_valid = subduction_min_distance_indexes[mask]\n",
    "indexes_valid = subduction_indexes[mask]\n",
    "\n",
    "# Plot pairs of matching points in the subduction zones\n",
    "# This includes plotting each marker path along the convergence direction and distance `d_m`\n",
    "ax.scatter(subduction_data_resampled.lon[mask], subduction_data_resampled.lat[mask], \n",
    "           marker=\".\", s=60, c='r', transform=ccrs.PlateCarree())  # Plot valid points in red\n",
    "ax.scatter(subduction_data_resampled.lon[mask_in], subduction_data_resampled.lat[mask_in], \n",
    "           marker=\".\", s=60, c='purple', transform=ccrs.PlateCarree())  # Plot invalid points in purple\n",
    "\n",
    "# Loop through each valid pair of points to plot their matching markers and connection lines\n",
    "for i in range(len(min_indexes_valid)):\n",
    "    try:\n",
    "        # Plot the matching points in blue\n",
    "        ax.scatter(subduction_data_resampled.lon[min_indexes_valid[i]], \n",
    "                   subduction_data_resampled.lat[min_indexes_valid[i]], \n",
    "                   marker=\".\", s=60, c='b', transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Draw a line between each query point and its matching subduction point\n",
    "        ax.plot([subduction_data_resampled.lon[indexes_valid[i]], subduction_data_resampled.lon[min_indexes_valid[i]]],\n",
    "                [subduction_data_resampled.lat[indexes_valid[i]], subduction_data_resampled.lat[min_indexes_valid[i]]],\n",
    "                c='c', transform=ccrs.PlateCarree())  # Plot lines in cyan\n",
    "    except KeyError:\n",
    "        pass  # Skip plotting if an index is missing (for debugging purposes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary keys represent subduction zone IDs, and the values specify\n",
    "\n",
    "from matplotlib.path import Path\n",
    "# the marker style, face color, and name associated with that ID.\n",
    "# this definition of snowflake initially has an error in the \"code\" part\n",
    "verts = [\n",
    "    (0., 0.),   # Center\n",
    "    (0.2, 0.6), # Upper arm\n",
    "    (0., 0.),   # Center\n",
    "    (0.4, 0.4), # Right diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (0.6, 0.2), # Right arm\n",
    "    (0., 0.),   # Center\n",
    "    (0.4, -0.4),# Right down diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (0.2, -0.6),# Bottom arm\n",
    "    (0., 0.),   # Center\n",
    "    (-0.4, -0.4),# Left down diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (-0.6, -0.2),# Left arm\n",
    "    (0., 0.),   # Center\n",
    "    (-0.4, 0.4),# Left diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (-0.2, 0.6),# Upper left arm\n",
    "]\n",
    "codes = [Path.MOVETO] + [Path.LINETO, Path.MOVETO] * 8 + [Path.MOVETO]\n",
    "snowflake = Path(verts, codes)\n",
    "\n",
    "# Define vertices for two equilateral triangles\n",
    "vertices = [\n",
    "    [0, 1], [-np.sqrt(3)/2, -0.5], [np.sqrt(3)/2, -0.5], [0, 1],  # First triangle\n",
    "    [0, -1], [-np.sqrt(3)/2, 0.5], [np.sqrt(3)/2, 0.5], [0, -1]   # Second triangle\n",
    "]\n",
    "# Flatten the vertices list for creating the Path\n",
    "vertices = np.array(vertices)\n",
    "# Define path codes (all 'LINETO' except the start 'MOVETO')\n",
    "codes = [Path.MOVETO] + [Path.LINETO] * (len(vertices) - 1)\n",
    "\n",
    "star_path = Path(vertices, codes)\n",
    "\n",
    "plot_options = \\\n",
    "{\n",
    "    903: {\"marker\": 'o',  \"markerfacecolor\": \"yellow\", \"name\": \"CAS\"},\n",
    "    511: {\"marker\": 's',  \"markerfacecolor\": \"yellow\", \"name\": \"ANDA-SUM\"},\n",
    "    801: {\"marker\": 'd',  \"markerfacecolor\": \"yellow\", \"name\": \"JAVA\"},\n",
    "    645: {\"marker\": snowflake,  \"markerfacecolor\": \"black\", \"name\": \"SULA\"},\n",
    "    602: {\"marker\": 'x',  \"markerfacecolor\": \"blue\", \"name\": \"LUZ\"},\n",
    "    608: {\"marker\": 's',  \"markerfacecolor\": 'c', \"name\": \"PHIL\"},\n",
    "    901: {\n",
    "        699: {\"marker\": '>',  \"markerfacecolor\": 'red', \"name\": \"MAR\"},\n",
    "        659: {\"marker\": 's',  \"markerfacecolor\": 'red', \"name\": \"IZU\"},\n",
    "        (601112.0, 601118.0): {\"marker\": '^',  \"markerfacecolor\": 'green', \"name\": \"JAP\"},\n",
    "        406:{\"marker\": 'v',  \"markerfacecolor\": 'green', \"name\": \"KUKAM\"},\n",
    "        111: {\"marker\": 'o',  \"markerfacecolor\": 'pink', \"name\": \"ALE-ALA\"},\n",
    "        (806, 821): {\"marker\": 'd',  \"markerfacecolor\": 'blue', \"name\": \"TON-KERM\"}\n",
    "        },\n",
    "    909: {\"marker\": star_path,  \"markerfacecolor\": 'c', \"name\": \"MEX\"},\n",
    "    911: {\"marker\": 'o',  \"markerfacecolor\": 'k', \"name\": \"PER-NCHI-JUAN-SCHI\"},\n",
    "    802: {\"marker\": 'd',  \"markerfacecolor\": 'k', \"name\": \"SSCHI-TBD\"},\n",
    "    201: {\n",
    "        2011:{\"marker\": '+',  \"markerfacecolor\": 'pink', \"name\": \"ANT\"},\n",
    "        815:{\"marker\": '*',  \"markerfacecolor\": 'r', \"name\": \"SAND\"}\n",
    "        },\n",
    "    1: {\"marker\": 'd',  \"markerfacecolor\": \"r\", \"name\": \"RYU\"}\n",
    "}\n",
    "\n",
    "# Create a figure and two subplots for plotting trench velocity data.\n",
    "# `gs` defines a 2x1 grid layout for the subplots.\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "gs = gridspec.GridSpec(3, 1)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax3 = fig.add_subplot(gs[2, 0])\n",
    "total_points_plotted = 0  # Variable to record the total number of plotted points.\n",
    "\n",
    "# Filter out rows with NaN values in the \"age\" column.\n",
    "mask_age = (~subduction_data_resampled[\"age\"].isna())\n",
    "total_points_plotted += len(subduction_data_resampled[mask_age])  # Count valid points.\n",
    "subduction_data_resampled_valid = subduction_data_resampled[mask_age]  # Store valid data.\n",
    "\n",
    "# Obtain a sorted list of unique subducting plate IDs from the valid data.\n",
    "# and round to nearest integar value\n",
    "unique_subducting_pids_0 = subduction_data_resampled_valid.subducting_pid.unique()\n",
    "\n",
    "unique_subducting_pids = []\n",
    "seen = set()\n",
    "for val in unique_subducting_pids_0:\n",
    "    if abs(val - round(val) < 0.1) and round(val) not in seen:\n",
    "           unique_subducting_pids.append(float(round(val)))\n",
    "           seen.add(round(val))\n",
    "    \n",
    "labels = []\n",
    "patches = []\n",
    "unique_subducting_pids.sort()  # Sort the unique subducting plate IDs.\n",
    "print(unique_subducting_pids)\n",
    "\n",
    "# Lookup and store subducting plate names based on their IDs.\n",
    "unique_subducting_names = []\n",
    "for i in range(len(unique_subducting_pids)):\n",
    "    subducting_pid = unique_subducting_pids[i]\n",
    "    # unique_subducting_names.append(GParseReconstruction.LookupNameByPid(int(subducting_id)))\n",
    "    unique_subducting_names.append(LookupNameByPid(trench_pids, name_lookups[\"trench_names\"], int(subducting_pid)))\n",
    "\n",
    "# Loop through each subducting plate ID and plot the corresponding trench velocity.\n",
    "k = 0\n",
    "for i in range(len(unique_subducting_pids)):\n",
    "    _name = unique_subducting_names[i]\n",
    "\n",
    "    subducting_id = unique_subducting_pids[i]\n",
    "    try:\n",
    "        plot_option_sub_dict = plot_options[int(subducting_id)]  # Get plot options for the ID.\n",
    "    except KeyError:\n",
    "        # If no specific plot option is found, use default settings.\n",
    "        continue # comment this to plot as TBD points\n",
    "        print(\"Id %s not found, marked as TBD\" % int(subducting_id))\n",
    "        plot_option_sub_dict = {\"marker\": 'o',  \"markerfacecolor\": None, \"name\": \"TBD\"}\n",
    "\n",
    "    # Make an output for the plotting function to loop over the trench ids\n",
    "    plot_trench_pids = None; plot_option_list = None\n",
    "    if 'name' in plot_option_sub_dict:\n",
    "        # A subduction contains a single trench\n",
    "        plot_trench_pids = [None]\n",
    "        plot_option_list = [plot_option_sub_dict.copy()]\n",
    "    else:\n",
    "        # A subduction contains multiple trenches\n",
    "        plot_trench_pids = []\n",
    "        plot_option_list = []\n",
    "        for key, value in plot_option_sub_dict.items():\n",
    "            plot_trench_pids.append(key)\n",
    "            plot_option_list.append(value.copy())\n",
    "\n",
    "    # Loop over the trench ids and plot the markers\n",
    "    for i_tr in range(len(plot_trench_pids)):\n",
    "        trench_pid = plot_trench_pids[i_tr]\n",
    "        plot_option = plot_option_list[i_tr]\n",
    "        # We want trench_pid options to be flexible.\n",
    "        # It could be a - a value; b - a range and c - multiple values\n",
    "        # d - None\n",
    "        # Create a mask for the current subducting plate and plot its trench velocity.\n",
    "        # We allow a variation of 0.1 from the integar value\n",
    "        # mask1 - match the subducting id\n",
    "        # mask2 - match the trench pid condition.\n",
    "        mask1 = (abs(subduction_data_resampled.subducting_pid - subducting_id) < 0.1)\n",
    "        if trench_pid is None:\n",
    "            mask = mask1\n",
    "        elif type(trench_pid) == float or type(trench_pid) == int:\n",
    "            mask = mask1 & (abs(subduction_data_resampled.trench_pid - trench_pid) < 0.1)\n",
    "        elif type(trench_pid) == list:\n",
    "            # mutiple values\n",
    "            mask2 = (abs(subduction_data_resampled.trench_pid - trench_pid[0]) < 0.1)\n",
    "            for trench_sub_pid in trench_pid[1:]:\n",
    "                mask2 = mask2 | (abs(subduction_data_resampled.trench_pid - trench_sub_pid) < 0.1)\n",
    "            mask = mask1 & mask2\n",
    "        elif type(trench_pid) == tuple:\n",
    "            # a range\n",
    "            assert(len(trench_pid) == 2)\n",
    "            mask2 = ((subduction_data_resampled.trench_pid >= trench_pid[0]) & (subduction_data_resampled.trench_pid <= trench_pid[1]))\n",
    "            mask = mask1 & mask2\n",
    "        else:\n",
    "            raise ValueError(\"Type of trench pid is wrong. Possible types are [None, float, int, list, dict]\")\n",
    "        ages = subduction_data_resampled_valid.age[mask]\n",
    "        trench_velocities = subduction_data_resampled_valid.trench_velocity[mask]\n",
    "        near_distances = subduction_data_resampled_valid.near_distance[mask]\n",
    "        _patch = ax1.plot(ages, trench_velocities,\\\n",
    "                marker=plot_option[\"marker\"], markerfacecolor=plot_option[\"markerfacecolor\"],\\\n",
    "                markeredgecolor='black', markersize=10, linestyle='', label=plot_option[\"name\"])[0]\n",
    "        _patch_d = ax3.plot(near_distances, trench_velocities,\\\n",
    "                marker=plot_option[\"marker\"], markerfacecolor=plot_option[\"markerfacecolor\"],\\\n",
    "                markeredgecolor='black', markersize=10, linestyle='', label=plot_option[\"name\"])[0]\n",
    "        patches.append(_patch)\n",
    "        print(k, \", subducting_id = \", subducting_id, \", trench_pid = \", trench_pid)\n",
    "        k += 1\n",
    "\n",
    "# Configure grid and legend for the second subplot.\n",
    "ax1.grid()\n",
    "ax3.grid()\n",
    "ax2.legend(handles=patches, bbox_to_anchor=(0.5, 0.5), loc='center', ncol=2, numpoints=1, frameon=False)\n",
    "\n",
    "# Output the total number of plotted points.\n",
    "print(\"Total plotted points: %d\" % total_points_plotted)\n",
    "\n",
    "# Set axis limits and labels for the first plot (trench velocity vs age).\n",
    "ax1.set_xlim([0, 160.0])\n",
    "ax1.set_ylim([-10.0, 10.0])\n",
    "ax3.set_ylim([-10.0, 10.0])\n",
    "ax1.set_xlabel(\"Age (Ma)\")\n",
    "ax1.set_ylabel(\"Trench Velocity Magnitude (cm/yr)\")\n",
    "ax3.set_ylabel(\"Trench Velocity Magnitude (cm/yr)\")\n",
    "\n",
    "# Save the figure to a PDF file with a name derived from the reconstruction parameters.\n",
    "fileout = os.path.join(HaMaGeoLib_DIR, \"dtemp\", \"gplate_subduction_zones\", \"subduction_distribution_t_%.2e_edge_%.2f_section_%.2f.pdf\"\\\n",
    "     % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "fig.savefig(fileout)\n",
    "print(\"figure saved: %s\" % fileout)\n",
    "\n",
    "# Save the subducting plate ID and names to a CSV file for future reference.\n",
    "csv_out = os.path.join(HaMaGeoLib_DIR, \"dtemp\", \"gplate_subduction_zones\", \"subduction_distribution_t_%.2e_edge_%.2f_section_%.2f.csv\"\\\n",
    "     % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "unique_data = {\n",
    "    \"pid\": unique_subducting_pids,\n",
    "    'name': unique_subducting_names\n",
    "}\n",
    "df_unique_data = pd.DataFrame(unique_data)\n",
    "df_unique_data.to_csv(csv_out)\n",
    "print(\"csv file saved: %s\" % csv_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-gplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
