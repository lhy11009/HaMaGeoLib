{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9fc7de",
   "metadata": {},
   "source": [
    "## Collision\n",
    "\n",
    "Notebook for collision modeling in ASPECT 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f107022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from shutil import rmtree, copy\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec, cm\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "# Derive the root of this package\n",
    "package_root = Path.cwd().resolve().parents[2]\n",
    "\n",
    "# Include this pakage\n",
    "HaMaGeoLib_DIR = \"/home/lochy/ASPECT_PROJECT/HaMaGeoLib\"\n",
    "if os.path.abspath(HaMaGeoLib_DIR) not in sys.path:\n",
    "    sys.path.append(os.path.abspath(HaMaGeoLib_DIR))\n",
    "from hamageolib.utils.exception_handler import my_assert\n",
    "import hamageolib.utils.plot_helper as plot_helper\n",
    "\n",
    "# Working directories\n",
    "local_Collision_dir = \"/mnt/lochy/ASPECT_DATA/Collision0\" # data directory\n",
    "# remote_ThDSubduction_dir = \"peloton:/group/billengrp-mpi-io/lochy/ThDSubduction\"\n",
    "assert(os.path.isdir(local_Collision_dir))\n",
    "\n",
    "# py_temp file and temperature results directory\n",
    "py_temp_dir = os.path.join(HaMaGeoLib_DIR, \"py_temp_files\")\n",
    "RESULT_DIR = os.path.join(HaMaGeoLib_DIR, 'results')\n",
    "os.makedirs(py_temp_dir, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "# paraview scripts directory\n",
    "SCRIPT_DIR = os.path.join(HaMaGeoLib_DIR, \"scripts\")\n",
    "\n",
    "today_date = datetime.datetime.today().strftime(\"%Y-%m-%d\") # Get today's date in YYYY-MM-DD format\n",
    "py_temp_file = os.path.join(py_temp_dir, f\"py_temp_{today_date}.sh\")\n",
    "\n",
    "if not os.path.exists(py_temp_file):\n",
    "    bash_header = \"\"\"#!/bin/bash\n",
    "# =====================================================\n",
    "# Script: py_temp.sh\n",
    "# Generated on: {date}\n",
    "# Description: Temporary Bash script created by Python\n",
    "# =====================================================\n",
    "\n",
    "\"\"\".format(date=today_date)\n",
    "    with open(py_temp_file, \"w\") as f:\n",
    "        f.write(bash_header)\n",
    "\n",
    "print(f\"File ensured at: {py_temp_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae18fd",
   "metadata": {},
   "source": [
    "# Pre Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a9ae9",
   "metadata": {},
   "source": [
    "## Continent Strength Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59647f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo_ct\n",
    "is_continent_strength = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c7135",
   "metadata": {},
   "source": [
    "### Set up Nnalysis of the Continental Profile\n",
    "\n",
    "Here we:\n",
    "1. read in profile of P, T of the coninental.\n",
    "2. parse the parameter file and read the rheological parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if is_continent_strength:\n",
    "\n",
    "    from gdmate.aspect.table import DepthAverageTable\n",
    "    from gdmate.aspect.io import parse_parameters_to_dict, parse_composition_entry, parse_entry_as_list\n",
    "\n",
    "    # Hard code the depth of layers, this is set up as the initial composition in the continental_extension.prm\n",
    "    depth_levels = [20e3, 40e3]  # crust_upper, crust_lower, mantle_lithosphere\n",
    "    \n",
    "    # Read the T, P profile from running the continental extension cookbook\n",
    "    # extract data at time 0.0\n",
    "    da_file = package_root/\"hamageolib/research/haoyuan_collision0/files/continental_extensiion/depth_average.txt\"\n",
    "    my_assert(da_file.is_file(), FileNotFoundError, \"%s doesn't exist\" % str(da_file))\n",
    "\n",
    "    my_table = DepthAverageTable(da_file)\n",
    "\n",
    "    profile_T = my_table.profile(time=0.0, field=\"temperature\")\n",
    "    profile_P = my_table.profile(time=0.0, field=\"adiabatic_pressure\")\n",
    "\n",
    "    # Read the prm file from the original continental extension cookbook\n",
    "    # Run parser\n",
    "    prm_path = package_root/\"hamageolib/research/haoyuan_collision0/files/continental_extensiion/continental_extension.prm\"\n",
    "    assert prm_path.exists(), f\"Test file not found: {prm_path}\"\n",
    "\n",
    "    with open(prm_path, \"r\") as fin:\n",
    "        params_dict = parse_parameters_to_dict(fin)\n",
    "\n",
    "    # Parse the compositions\n",
    "    compositional_field_names = parse_entry_as_list(params_dict[\"Compositional fields\"][\"Names of fields\"])\n",
    "    compositional_field_types = parse_entry_as_list(params_dict[\"Compositional fields\"][\"Types of fields\"])\n",
    "\n",
    "    chemical_composition_list = []\n",
    "    for i, type in enumerate(compositional_field_types):\n",
    "        if type == \"chemical composition\":\n",
    "            chemical_composition_list.append(compositional_field_names[i])\n",
    "\n",
    "    # Parse the rheological parameters\n",
    "    material_dict = params_dict[\"Material model\"][\"Visco Plastic\"]\n",
    "\n",
    "    disl_A_dict = parse_composition_entry(material_dict[\"Prefactors for dislocation creep\"])\n",
    "    disl_n_dict = parse_composition_entry(material_dict[\"Stress exponents for dislocation creep\"])\n",
    "    disl_E_dict = parse_composition_entry(material_dict[\"Activation energies for dislocation creep\"])\n",
    "    disl_V_dict = parse_composition_entry(material_dict[\"Activation volumes for dislocation creep\"])\n",
    "\n",
    "    dislocation_aspect = {}\n",
    "    for composition in chemical_composition_list:\n",
    "        dislocation_aspect_comp = {\n",
    "            'A': float(disl_A_dict[composition]), \n",
    "            'd': float(material_dict.get(\"Grain size\", 1e-3)), \n",
    "            'n': float(disl_n_dict[composition]), \n",
    "            'm': 0.0, \n",
    "            'E': float(disl_E_dict[composition]), \n",
    "            'V': float(disl_V_dict[composition])\n",
    "        }\n",
    "        dislocation_aspect[composition] = dislocation_aspect_comp\n",
    "    \n",
    "    friction_angle = float(material_dict[\"Angles of internal friction\"])\n",
    "    cohesion = float(material_dict[\"Cohesions\"])\n",
    "\n",
    "    # print parse results\n",
    "    print(\"chemical_composition_list: \")\n",
    "    print(chemical_composition_list)\n",
    "    print(\"dislocation_aspect: \")\n",
    "    print(dislocation_aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebf21f",
   "metadata": {},
   "source": [
    "### Construct the Strength Profile\n",
    "\n",
    "Here we:\n",
    "1. Assign a depth array and interpolate the P, T profile from on we read in the last step.\n",
    "2. Assume constant strain rate and compute the viscosity.\n",
    "3. Use composite regime to combine different rheologic mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_continent_strength:\n",
    "\n",
    "    from hamageolib.research.haoyuan_2d_subduction.legacy_tools import CoulumbYielding, CreepRheologyInAspectViscoPlastic\n",
    "\n",
    "    # compute at this constant strain rate\n",
    "    strain_rate = 1e-14\n",
    "\n",
    "    # yield stress of brittle yielding\n",
    "    depths = np.arange(0, 101e3, 1e3)\n",
    "    Ts = np.interp(depths, profile_T[\"depth\"].to_numpy(), profile_T[\"temperature\"].to_numpy())\n",
    "    Ps = np.interp(depths, profile_P[\"depth\"].to_numpy(), profile_P[\"adiabatic_pressure\"].to_numpy())\n",
    "\n",
    "    friction = np.tan(friction_angle*np.pi/180.0)\n",
    "    taus_brittle = CoulumbYielding(Ps, cohesion, friction)\n",
    "    eta_brittle = taus_brittle / 2.0 / strain_rate\n",
    "\n",
    "    # dislocation creep\n",
    "    # use mask to compute for each compositions\n",
    "    eta_dislocation = np.zeros(depths.size)\n",
    "\n",
    "    composition = \"crust_upper\"\n",
    "    mask_c = (depths <= depth_levels[0])\n",
    "    eta_dislocation[mask_c] = CreepRheologyInAspectViscoPlastic(dislocation_aspect[composition], strain_rate, Ps[mask_c], Ts[mask_c])\n",
    "    \n",
    "    composition = \"crust_lower\"\n",
    "    mask_c = ((depths > depth_levels[0]) & (depths <= depth_levels[1]))\n",
    "    eta_dislocation[mask_c] = CreepRheologyInAspectViscoPlastic(dislocation_aspect[composition], strain_rate, Ps[mask_c], Ts[mask_c])\n",
    "    \n",
    "    composition = \"mantle_lithosphere\"\n",
    "    mask_c = (depths > depth_levels[1])\n",
    "    eta_dislocation[mask_c] = CreepRheologyInAspectViscoPlastic(dislocation_aspect[composition], strain_rate, Ps[mask_c], Ts[mask_c])\n",
    "\n",
    "    # placeholder for diffusion creep\n",
    "    eta_diffusion = np.full(depths.shape, np.inf)\n",
    "\n",
    "    # placeholder for Peierls creep\n",
    "    eta_peierls = np.full(depths.shape, np.inf)\n",
    "    taus_peierls = np.full(depths.shape, np.inf)\n",
    "\n",
    "    # combine diffusion and dislocation\n",
    "    eta_dfds = 1.0 / (1.0 / eta_diffusion + 1.0 / eta_dislocation)\n",
    "    taus_dfds = 2 * strain_rate * eta_dfds\n",
    "\n",
    "    # combine deformation mechanisms follow DP-yielding\n",
    "    eta = 1.0 / (1.0/eta_brittle + 1.0/eta_peierls + 1.0/eta_dfds)\n",
    "    eta1 = np.minimum(eta_peierls, eta_dfds)  # minimum\n",
    "    eta1 = np.minimum(eta_brittle, eta1)\n",
    "    eta2 = np.minimum(eta_brittle, 1.0 / (1.0 / eta_peierls + 1.0/eta_dfds)) # DP yield\n",
    "    eta3 = np.minimum(eta_brittle, 1.0/(1.0 / np.minimum(eta_peierls, eta_dislocation) + 1.0 / eta_diffusion)) # competing Peierls and Dislocation\n",
    "    eta_nopc = np.minimum(eta_brittle, eta_dfds)\n",
    "    taus = 2 * strain_rate * eta\n",
    "    taus1 = 2 * strain_rate * eta1\n",
    "    taus2 = 2 * strain_rate * eta2\n",
    "    taus3 = 2 * strain_rate * eta3\n",
    "    taus_nopc = 2 * strain_rate * eta_nopc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f7ac0",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cf677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo_ct\n",
    "print(taus_brittle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c867410",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_continent_strength:\n",
    "\n",
    "    from matplotlib import gridspec, rcdefaults \n",
    "    from matplotlib.ticker import MultipleLocator\n",
    "    from hamageolib.utils.plot_helper import scale_matplotlib_params\n",
    "\n",
    "    # Retrieve the default color cycle\n",
    "    default_colors = [color['color'] for color in plt.rcParams['axes.prop_cycle']]\n",
    "\n",
    "    # Scaling parameters for plots.\n",
    "    scaling_factor = 1.6  # General scaling factor for the plot size.\n",
    "    font_scaling_multiplier = 1.5  # Extra scaling for fonts.\n",
    "    legend_font_scaling_multiplier = 0.5  # Scaling for legend fonts.\n",
    "    line_width_scaling_multiplier = 2.0  # Extra scaling for line widths\n",
    "    n_minor_ticks = 4  # number of minor ticks between two major ones\n",
    "\n",
    "\n",
    "    # Scale matplotlib parameters based on specified factors.\n",
    "    scale_matplotlib_params(\n",
    "        scaling_factor, \n",
    "        font_scaling_multiplier=font_scaling_multiplier,\n",
    "        legend_font_scaling_multiplier=legend_font_scaling_multiplier,\n",
    "        line_width_scaling_multiplier=line_width_scaling_multiplier\n",
    "    )\n",
    "\n",
    "    # Update font settings for compatibility with publishing tools like Illustrator.\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Times New Roman',\n",
    "        'pdf.fonttype': 42,\n",
    "        'ps.fonttype': 42\n",
    "    })\n",
    "\n",
    "    # plot\n",
    "    # 1. shear stress vs depth\n",
    "    fig = plt.figure(tight_layout=True, figsize=(12, 10))\n",
    "    gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    ax.plot(taus2/1e6, depths/1e3, label=\"Composite, DP yield\")\n",
    "    ax.plot(taus_nopc/1e6, depths/1e3, label=\"Composite (no peierls)\")\n",
    "    ax.plot(taus_brittle/1e6, depths/1e3, 'b--', label=\"Brittle\")\n",
    "    ax.plot(taus_peierls, depths/1e3, 'c--', label=\"Peierls\") # peierls is MPa\n",
    "    ax.plot(taus_dfds/1e6, depths/1e3, 'g--', label=\"Diff-Disl\")\n",
    "\n",
    "    ax.set_xlabel(\"Stress (MPa)\")\n",
    "    ax.set_xlim([0, 1000.0])\n",
    "    x_tick_interval = 250.0\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(x_tick_interval))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(x_tick_interval/(n_minor_ticks+1)))\n",
    "\n",
    "    ax.set_ylabel(\"Depth (km)\")\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.invert_yaxis()\n",
    "    y_tick_interval = 25.0\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(y_tick_interval))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(y_tick_interval/(n_minor_ticks+1)))\n",
    "\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "    # 2. viscosity vs depth\n",
    "    ax = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    ax.plot(np.log10(eta2), depths/1e3, label=\"Composite, DP yield\")\n",
    "    ax.plot(np.log10(eta_brittle), depths/1e3, 'b--', label=\"Brittle\")\n",
    "    ax.plot(np.log10(eta_peierls), depths/1e3, 'c--', label=\"Peierls\")\n",
    "    ax.plot(np.log10(eta_dfds), depths/1e3, 'g--', label=\"Diff-Disl\")\n",
    "\n",
    "    ax.set_xlabel(\"log10(Viscosity) (Pa s)\")\n",
    "    ax.set_xlim([18.0, 24.0])\n",
    "    x_tick_interval = 1.0\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(x_tick_interval))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(x_tick_interval/(n_minor_ticks+1)))\n",
    "\n",
    "    ax.set_ylabel(\"Depth (km)\")\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.invert_yaxis()\n",
    "    y_tick_interval = 25.0\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(y_tick_interval))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(y_tick_interval/(n_minor_ticks+1)))\n",
    "\n",
    "    ax.grid()\n",
    "    ax.set_title(\"strain rate = %.2e\" % strain_rate)\n",
    "\n",
    "    # 3. shear stress vs depth\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    ax.plot(taus2/1e6, depths/1e3, label=\"Composite, DP yield\")\n",
    "    ax.plot(taus/1e6, depths/1e3, label=\"Composite, SL yield\")\n",
    "    ax.plot(taus_nopc/1e6, depths/1e3, label=\"Composite (no peierls)\")\n",
    "    ax.plot(taus1/1e6, depths/1e3, label=\"Minimization\")\n",
    "    ax.plot(taus3/1e6, depths/1e3, label=\"Min(Peierls, dislocation)\")\n",
    "\n",
    "    ax.set_xlabel(\"Stress (MPa)\")\n",
    "    ax.set_xlim([0, 1000.0])\n",
    "    x_tick_interval = 250.0\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(x_tick_interval))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(x_tick_interval/(n_minor_ticks+1)))\n",
    "\n",
    "    ax.set_ylabel(\"Depth (km)\")\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.invert_yaxis()\n",
    "    y_tick_interval = 25.0\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(y_tick_interval))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(y_tick_interval/(n_minor_ticks+1)))\n",
    "\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "    rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bed9f",
   "metadata": {},
   "source": [
    "# Create Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record of configurations\n",
    "# thicker weak layer including \"gabbro\" with a higher viscosity\n",
    "            #   \"weak_layer_compositions\": [\"MORB\", \"sediment\", \"gabbro\"], # weak layer\n",
    "            #   \"weak_layer_viscosity\": 1e20, # weak layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4639f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_create_new_case = False\n",
    "\n",
    "if is_create_new_case:\n",
    "\n",
    "    import json\n",
    "    import copy\n",
    "    from gdmate.aspect.config_engine import RuleEngine\n",
    "    from gdmate.aspect.builtin_rules import CasePathRule, InitialStepRule\n",
    "    from gdmate.aspect.io import parse_parameters_to_dict, save_parameters_from_dict\n",
    "    from hamageolib.research.haoyuan_collision0.config import CaseNameFromVariables, GeometryRule, PostProcessorRule, RemoveFluidRule, RemovePeridotiteRule,\\\n",
    "        RheologyRule, WeakLayerRule, SlabRule, SolverRule, PrescribConditionRule\n",
    "    \n",
    "    root_dir = Path(\"/mnt/lochy/ASPECT_DATA/Collision0/collision_setup\")\n",
    "    \n",
    "    template_prm = package_root/\"hamageolib/research/haoyuan_collision0/files/01112026/post_compressible_test.prm\"\n",
    "    template_wb = package_root/\"hamageolib/research/haoyuan_collision0/files/01112026/original.wb\"\n",
    "\n",
    "    # Make the root dir\n",
    "    root_dir.mkdir(exist_ok=True) \n",
    "    \n",
    "    # Initiate rule engine\n",
    "    ruleEngine = RuleEngine([PostProcessorRule(), CasePathRule(), RemoveFluidRule(), RemovePeridotiteRule(), \n",
    "                             SlabRule(), GeometryRule(), RheologyRule(), WeakLayerRule(), PrescribConditionRule(), \n",
    "                             SolverRule()])\n",
    "    ruleEngineInitalStep = RuleEngine([InitialStepRule()])\n",
    "    \n",
    "    # Read prm and wb templates\n",
    "    assert(template_prm.is_file())\n",
    "    with template_prm.open('r') as fin:\n",
    "        prm_dict = parse_parameters_to_dict(fin, format_entry=True)\n",
    "    assert(template_wb.is_file())\n",
    "    with template_wb.open('r') as fin:\n",
    "        wb_dict = json.load(fin)\n",
    "    \n",
    "    # debug the formate_entry option\n",
    "    foo = prm_dict[\"Compositional fields\"][\"Mapped particle properties\"]\n",
    "    \n",
    "    # Apply case configuration rules\n",
    "    config = {\"use_my_setup_of_postprocess\": True,\n",
    "            #   \"remove_fluid\": True,  # remove fluid\n",
    "            #   \"remove_fluid_compositions\": [\"porosity\", \"bound_fluid\"], # remove fluid\n",
    "              \"remove_peridotite\": True, # remove peridotite\n",
    "              \"domain_depth\": 1000e3, \"repetition_length\": 500e3, \"use_isosurfaces\": True, # geometry\n",
    "              \"use_my_setup_of_rheology\": True, # rheology\n",
    "              \"viscosity_range\": [2.5e19, 2.5e23], # rheology\n",
    "              \"use_safer_options\": True, # rheology, set adiabatic pressure and cutoff negative temperature\n",
    "            #   \"weak_layer_compositions\": [\"MORB\", \"sediment\"], # weak layer, normal setting\n",
    "              \"weak_layer_compositions\": [\"MORB\", \"sediment\", \"gabbro\"], # weak layer, setting thicker weak layers\n",
    "              \"force_weak_layer_max_refinement\": True, # weak layer\n",
    "              \"slab_layer_compositions\": [\"sediment\", \"MORB\", \"gabbro\"], # slab\n",
    "              \"slab_layer_depths\": [0.0, 4e3, 7.5e3, 15e3], # slab & remove peridotite\n",
    "              \"stokes_solver_type\": \"block GMG\",\n",
    "              \"skip_expensive_stokes\": True, # solver\n",
    "              \"max_nonlinear_iterations\": 40, # solver\n",
    "              \"linear_solver_tolerance\": 5e-3, # solver\n",
    "              \"number_of_cheap_Stokes_solver_steps\": 60, # solver\n",
    "              \"GMRES_solver_restart_length\": 100, # solver\n",
    "              \"prescribe_subducting_plate_velocity\": True, # prescribed condition\n",
    "              \"prescribe_subducting_plate_velocity_region_method\": \"relative_to_hinge\", # prescribed condition\n",
    "              \"prescribe_subducting_plate_velocity_velocity_method\":\"spreading_velocity\", # prescribed condition\n",
    "              \"prescribe_subducting_plate_velocity_hinge_relative_distance\": 1000e3, # prescribed condition\n",
    "              \"prescribe_subducting_plate_velocity_length\": 500e3, # prescribed condition\n",
    "              \"prescribe_subducting_plate_velocity_depth_range\": [20e3, 40e3], # prescribed condition\n",
    "              \"convergence_rate\": 0.05, # prescribed condition\n",
    "              }\n",
    "\n",
    "    contexts, documentation = ruleEngine.apply_all(config, prm_dict, wb_dict)\n",
    "\n",
    "    # create case directory based on name patterns\n",
    "    # check existing ones and ask user permission to proceed\n",
    "    variables = ruleEngine.get_required_variables(config)\n",
    "    case_name = CaseNameFromVariables(variables)\n",
    "    case_dir = root_dir/case_name\n",
    "\n",
    "    if case_dir.is_dir():\n",
    "        ans = input(\"Director %s already exist, overwrite [y/n]?\" % str(case_dir))\n",
    "        if ans == \"y\":\n",
    "            print(\"Going to overwrite the pre-existing case.\")\n",
    "        else:\n",
    "            print(\"Cell execution stopped.\")\n",
    "            raise SystemExit\n",
    "        \n",
    "    case_dir.mkdir(exist_ok=True)\n",
    "    case_prm = case_dir/\"case.prm\"\n",
    "    case_prm_initial = case_dir/\"case_ini.prm\"\n",
    "    case_wb = case_dir/\"case.wb\"\n",
    "    \n",
    "    # write case prm file\n",
    "    with case_prm.open('w') as fout:\n",
    "        save_parameters_from_dict(fout, prm_dict)\n",
    "    with case_wb.open('w') as fout:\n",
    "        json.dump(wb_dict, fout, indent=4)\n",
    "    print(\"Saved prm file %s\" % str(case_prm))\n",
    "    print(\"Saved wb file %s\" % str(case_wb))\n",
    "    \n",
    "    # write the initial step prm file\n",
    "    config_initial = {\"n_trivial_step\": 1}\n",
    "    prm_dict_initial = copy.deepcopy(prm_dict)\n",
    "    _, _  = ruleEngineInitalStep.apply_all(config_initial, prm_dict_initial, wb_dict)\n",
    "    with case_prm_initial.open('w') as fout:\n",
    "        save_parameters_from_dict(fout, prm_dict_initial)\n",
    "    print(\"Saved prm file %s\" % str(case_prm_initial))\n",
    "\n",
    "    # save documentation\n",
    "    doc_dir = case_dir/\"doc\"\n",
    "    if not doc_dir.is_dir():\n",
    "        doc_dir.mkdir()\n",
    "    \n",
    "    full_doc_path = doc_dir/\"full_doc.md\"\n",
    "    doc_md = ruleEngine.render_docs_markdown(documentation)\n",
    "    with full_doc_path.open(\"w\") as fout:\n",
    "        fout.write(doc_md)\n",
    "    print(\"Saved full documentation %s\" % str(full_doc_path))\n",
    "    \n",
    "    full_table_path = doc_dir/\"full_table.md\"\n",
    "    table_md = ruleEngine.render_docs_table(documentation)\n",
    "    with full_table_path.open(\"w\") as fout:\n",
    "        fout.write(table_md)\n",
    "    print(\"Saved full table %s\" % str(full_table_path))\n",
    "\n",
    "    full_json_path = doc_dir/\"full.json\"\n",
    "    with full_json_path.open(\"w\") as fout:\n",
    "        json.dump(documentation, fout, indent=2)\n",
    "    print(\"Saved full json %s\" % str(full_json_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85105b8",
   "metadata": {},
   "source": [
    "# Post Process\n",
    "\n",
    "We now begin processing the simulation cases.\n",
    "The first step is to enumerate the case directories and confirm that all specified paths exist on the filesystem.\n",
    "\n",
    "Options:\n",
    "\n",
    "* do_post_process - control the run of the following code blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_post_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b811cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_post_process:\n",
    "    # placeholder for a 3-D case\n",
    "    case_name = None \n",
    "\n",
    "    # Normally, I don't need to specify name of the file, they are case.prm and case.wb be default\n",
    "    prm_basename_2d = \"case.prm\"; wb_basename_2d = \"case.wb\"; output_directory=\"output\"\n",
    "    # Initial test from Fritz\n",
    "    # case_name_2d = \"collision_test/Fritz_test0\"; prm_basename_2d = \"init_compressible_test.prm\"; wb_basename_2d = \"original.wb\"; output_directory=\"isentropic_adiabat_new_boundary\"\n",
    "\n",
    "    # Test with shallow domain\n",
    "    case_name_2d = \"collision_setup/D1000_WLCG_WLV1.0e+20\"\n",
    "    \n",
    "    # Test with shallow domain, removed fluid\n",
    "    # case_name_2d = \"collision_test/test0\"; prm_basename_2d = \"case_ini.prm\"\n",
    "\n",
    "    local_dir = None; local_dir_2d = None\n",
    "    if case_name is not None:\n",
    "        local_dir = os.path.join(local_Collision_dir, case_name)\n",
    "        assert(os.path.isdir(local_dir))\n",
    "        print(\"local_dir:\\n\\t\", local_dir)\n",
    "        subprocess.run(['mkdir', '-p', '%s/img/pv_outputs' % local_dir])\n",
    "    if case_name_2d is not None:\n",
    "        local_dir_2d = os.path.join(local_Collision_dir, case_name_2d)\n",
    "        assert(os.path.isdir(local_dir_2d))\n",
    "        print(\"local_dir_2d:\\n\\t\", local_dir_2d)\n",
    "        subprocess.run(['mkdir', '-p', '%s/img/pv_outputs' % local_dir_2d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19f1ab",
   "metadata": {},
   "source": [
    "## Generate Paraview script\n",
    "\n",
    "We apply a combined workflow with **ParaView**.  \n",
    "In this notebook, we generate the processing script.  \n",
    "We then use ParaView to refine styling and export the final plots.\n",
    "\n",
    "options\n",
    "\n",
    "- `is_prepare_for_plot` — Generate the **ParaView** Python script (no data pre-processing).  \n",
    "- `is_process_pyvista_for_plot` — Pre-process results with **PyVista** before exporting the ParaView script, then proceed in ParaView.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_prepare_for_plot = True\n",
    "is_process_pyvista_for_plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_post_process and is_prepare_for_plot:\n",
    "\n",
    "\n",
    "    from hamageolib.research.haoyuan_collision0.case_options import CASE_OPTIONS_TWOD\n",
    "    from hamageolib.research.haoyuan_collision0.post_process import ProcessVtuFileTwoDStep, GenerateParaviewScript\n",
    "\n",
    "    # check again case directory exists    \n",
    "    assert(local_dir_2d is not None)\n",
    "\n",
    "    # options \n",
    "    graphical_step = 0\n",
    "\n",
    "    # original script\n",
    "    ofile_list = [\"collision0.py\"]; require_base=True\n",
    "\n",
    "    # automatically read case configurations\n",
    "    Case_Options_2d = CASE_OPTIONS_TWOD(local_dir_2d, case_file=prm_basename_2d, wb_basename=wb_basename_2d, output_directory=output_directory)\n",
    "    Case_Options_2d.Interpret(step=graphical_step)\n",
    "    Case_Options_2d.SummaryCaseVtuStep(os.path.join(local_dir_2d, \"summary.csv\"))\n",
    "\n",
    "    # the index of the vtu file depends on whether all the intial adaptive steps are output\n",
    "    pvtu_step = Case_Options_2d.get_pvtu_step(graphical_step)\n",
    "\n",
    "    ProcessVtuFileTwoDStep(local_dir_2d, pvtu_step, Case_Options_2d)\n",
    "\n",
    "    GenerateParaviewScript(local_dir_2d, Case_Options_2d, ofile_list, require_base=require_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e3aab",
   "metadata": {},
   "source": [
    "## Automize figure finalization\n",
    "\n",
    "The figures generated from the previous **ParaView** script can be finalized using the following code block.\n",
    "We would use a \"frame\" figure generated in Adobe illustrator and overlay that on the ParaView plot.\n",
    "\n",
    "Options:\n",
    "- `file_name` — Name of the plot to finalize.\n",
    "- `_time` — Model time associated with the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa534ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo_collision\n",
    "finalize_visual = False\n",
    "\n",
    "if do_post_process and finalize_visual:\n",
    "    \n",
    "    from hamageolib.research.haoyuan_collision0.post_process import finalize_visualization_2d_11022025 \n",
    "\n",
    "    # options for file_name and time\n",
    "    file_name = \"full_domain_viscosity\" # \"full_domain_viscosity\", \"full_model_temperature\"\n",
    "    _time = 2.9014e+06\n",
    "\n",
    "    frame_png_file_with_ticks = \"/home/lochy/Documents/papers/documented_files/collision/full_domain_frame_trans-01.png\"\n",
    "    output_image_file = finalize_visualization_2d_11022025(local_dir_2d, file_name, _time, frame_png_file_with_ticks, add_time=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ece073",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b0e29",
   "metadata": {},
   "source": [
    "### 2-d case, basic\n",
    "\n",
    "First generate the plotting scripts, then run them in the terminal. Assemble the final animation only after all figures have been generated. In practice, this section may need to be executed multiple times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_2d_case_basic = False\n",
    "debug_step0_animate_2d_case_basic = True\n",
    "generate_paraview_scripts_for_animate_2d_case_basic = True\n",
    "\n",
    "if animate_2d_case_basic:\n",
    "    \n",
    "    from hamageolib.research.haoyuan_collision0.case_options import CASE_OPTIONS_TWOD\n",
    "    from hamageolib.research.haoyuan_collision0.post_process import ProcessVtuFileTwoDStep, GenerateParaviewScript\n",
    "\n",
    "    # Assign a time interval for animation\n",
    "    time_interval = 0.5e6\n",
    "    animation_name= \"ani_basic\"\n",
    "    max_depth = \"1500\"\n",
    "\n",
    "    # Apply case options\n",
    "    Case_Options_2d = CASE_OPTIONS_TWOD(local_dir_2d, case_file=prm_basename_2d, wb_basename=wb_basename_2d, output_directory=output_directory)\n",
    "    Case_Options_2d.Interpret()\n",
    "    # Case_Options_2d.SummaryCaseVtuStep(os.path.join(local_dir_2d, \"summary.csv\"))\n",
    "    # Case_Options_2d.SummaryCaseVtuStepExport(os.path.join(local_dir_2d, \"summary.csv\"))\n",
    "    resampled_df = Case_Options_2d.resample_visualization_df(time_interval)\n",
    "    graphical_steps = resampled_df[\"Vtu step\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6fbe7",
   "metadata": {},
   "source": [
    "#### Generate **ParaView** scripts stepwise\n",
    "\n",
    "Optional: generate stepwise **ParaView** scripts controlled by `generate_paraview_scripts_for_animate_2d_case_basic` option.\n",
    "After this step, run `py_temp.py` script in a terminal to produce the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if animate_2d_case_basic and generate_paraview_scripts_for_animate_2d_case_basic: \n",
    "\n",
    "    # Open py_temp_file for output\n",
    "    fout = open(py_temp_file, 'w')\n",
    "    assert(fout)\n",
    "    fout.write(\"#!/bin/bash\\n\")\n",
    "\n",
    "    # Run stepwise\n",
    "    print(\"Start generating paraview scripts\")\n",
    "    for i, _time in enumerate(resampled_df[\"Time\"].values):\n",
    "\n",
    "        # debug run step 0\n",
    "        if debug_step0_animate_2d_case_basic:\n",
    "            if i > 0:\n",
    "                break\n",
    "\n",
    "        # Stepwise configurations \n",
    "        _time = float(_time)\n",
    "        time_rounded = round(_time / float(resampled_df.attrs[\"Time between graphical output\"]))\\\n",
    "              * float(resampled_df.attrs[\"Time between graphical output\"])\n",
    "        step = int(graphical_steps[i])\n",
    "        print(\"\\tGenerating paraview scripts for step = %d, time = %.4e\" % (step, time_rounded))\n",
    "\n",
    "        # Assign the script to use\n",
    "        py_script = \"collision0.py\"\n",
    "\n",
    "        # Make the directory to hold the scripts\n",
    "        ps_dir = os.path.join(local_dir_2d, 'paraview_scripts')\n",
    "        if not os.path.isdir(ps_dir):\n",
    "            os.mkdir(ps_dir) \n",
    "        odir = os.path.join(ps_dir, \"stepwise\")\n",
    "        if not os.path.isdir(odir):\n",
    "            os.mkdir(odir)\n",
    "\n",
    "        # Apply stepwise configuration\n",
    "        Case_Options_2d.options['ANIMATION'] = \"True\"\n",
    "        Case_Options_2d.options['GRAPHICAL_STEPS'] = [step]\n",
    "        Case_Options_2d.options['GRAPHICAL_TIMES'] = [time_rounded]\n",
    "\n",
    "        ofile = os.path.join(odir, 'slab_%d.py' % (step))\n",
    "        paraview_script = os.path.join(SCRIPT_DIR, 'paraview_scripts', 'Collision0', py_script)\n",
    "        paraview_script_base = os.path.join(SCRIPT_DIR, 'paraview_scripts', 'base.py')\n",
    "        Case_Options_2d.read_contents(paraview_script_base, paraview_script)\n",
    "\n",
    "        # Save script\n",
    "        Case_Options_2d.substitute()\n",
    "        Case_Options_2d.save(ofile)\n",
    "\n",
    "        # Write to py_temp file\n",
    "        fout.write(\"pvpython %s\\n\" % ofile)\n",
    "\n",
    "    # Finish writting to py_temp file\n",
    "    fout.close()\n",
    "    subprocess.run([\"chmod\", \"+x\", py_temp_file])\n",
    "    print(\"saved file: %s\" % py_temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b6736",
   "metadata": {},
   "source": [
    "#### Finalize plot from **ParaView**\n",
    "\n",
    "Ensure the previous step has completed. Use the following cell to finalize plots generated by **ParaView**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_framing = False\n",
    "\n",
    "if animate_2d_case_basic:\n",
    "    \n",
    "    from hamageolib.research.haoyuan_collision0.post_process import finalize_visualization_2d_11022025\n",
    "    from hamageolib.utils.plot_helper import extract_image_by_size\n",
    "\n",
    "    # file types\n",
    "    file_name_list = [\"full_domain_viscosity\", \"full_domain_temperature\", \"full_domain_density\"]\n",
    "    prep_dir = os.path.join(local_dir_2d, \"img\", \"prep\")\n",
    "    if not os.path.isdir(prep_dir):\n",
    "        os.mkdir(prep_dir)\n",
    "\n",
    "    print(\"Start Finalizing Plots\")\n",
    "    for i, _time in enumerate(resampled_df[\"Time\"].values):\n",
    "\n",
    "        # debug run step 0\n",
    "        if debug_step0_animate_2d_case_basic:\n",
    "            if i > 0:\n",
    "                break\n",
    "\n",
    "        # Stepwise configurations \n",
    "        _time = resampled_df[\"Time\"].values[i]\n",
    "        time_rounded = round(_time / float(resampled_df.attrs[\"Time between graphical output\"]))\\\n",
    "              * float(resampled_df.attrs[\"Time between graphical output\"])\n",
    "        step = graphical_steps[i]\n",
    "        print(\"\\tFinalizing plots for step = %d, time = %.4e\" % (step, time_rounded))\n",
    "\n",
    "        for file_name in file_name_list:\n",
    "            if need_framing:\n",
    "                # framing with plot with preprepared frames\n",
    "                frame_png_file_with_ticks = \"/home/lochy/Documents/papers/documented_files/collision/full_domain_frame_trans_frame-01.png\"\n",
    "                output_image_file = finalize_visualization_2d_11022025(local_dir_2d, file_name, time_rounded, frame_png_file_with_ticks, add_time=False)\n",
    "            else:\n",
    "                # if not only check file existence\n",
    "                eps_file = os.path.join(local_dir_2d, \"img\", \"pv_outputs\", \"%s_t%.4e.eps\" % (file_name, time_rounded))\n",
    "                pdf_file = os.path.join(local_dir_2d, \"img\", \"pv_outputs\", \"%s_t%.4e.pdf\" % (file_name, time_rounded))\n",
    "\n",
    "\n",
    "                if (not os.path.isfile(eps_file)) and (not os.path.isfile(pdf_file)):\n",
    "                    raise FileNotFoundError(f\"Neither the EPS nor pdf exists: {eps_file}, {pdf_file}\")\n",
    "                \n",
    "                # if os.path.isfile(pdf_file):\n",
    "                #     target_size = (1350, 704)  # Desired image dimensions in pixels\n",
    "                #     crop_box = (0, 0, 1000, 1000)  # Optional crop box\n",
    "                #     full_image_path = extract_image_by_size(pdf_file, target_size, prep_dir, crop_box)\n",
    "                #     copy(full_image_path, os.path.join(prep_dir, \"%s_t%.4e.png\" % (file_name, time_rounded)))\n",
    "                #     print(\"Saved file as %s\" % os.path.join(prep_dir, \"%s_t%.4e.png\" % (file_name, time_rounded)))\n",
    "                copy(os.path.join(local_dir_2d, \"img\", \"pv_outputs\", \"%s_t%.4e.png\" % (file_name, time_rounded)),\\\n",
    "                    os.path.join(prep_dir, \"%s_t%.4e.png\" % (file_name, time_rounded)))\n",
    "                print(\"Saved file as %s\" % os.path.join(prep_dir, \"%s_t%.4e.png\" % (file_name, time_rounded)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4cf7d",
   "metadata": {},
   "source": [
    "#### Assemble and make animation\n",
    "\n",
    "Using the generated figures (e.g., linear plots and finalized figures from **ParaView**):\n",
    "\n",
    "1. Assemble them stepwise and export one combined figure per step.\n",
    "2. Create an `.avi` animation from the combined figures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if animate_2d_case_basic:\n",
    "\n",
    "    print(\"Start making animation\")\n",
    "\n",
    "    # Load modules\n",
    "    from hamageolib.research.haoyuan_2d_subduction.workflow_scripts import create_avi_from_images\n",
    "\n",
    "    # Initiation\n",
    "    ani_file_paths = [] # path of the figures\n",
    "\n",
    "    # Loop the steps to get job done\n",
    "    for i, _time in enumerate(resampled_df[\"Time\"].values):\n",
    "\n",
    "        # debug run step 0\n",
    "        if debug_step0_animate_2d_case_basic:\n",
    "            if i > 0:\n",
    "                break\n",
    "\n",
    "        # do this if there is error at the last step\n",
    "        if i == resampled_df[\"Time\"].values.size - 1:\n",
    "            break\n",
    "\n",
    "        # Stepwise configurations \n",
    "        _time = resampled_df[\"Time\"].values[i]\n",
    "        time_rounded = round(_time / float(resampled_df.attrs[\"Time between graphical output\"]))\\\n",
    "              * float(resampled_df.attrs[\"Time between graphical output\"])\n",
    "        step = graphical_steps[i]\n",
    "        print(\"\\tAssembling plots for step = %d, time = %.4e\" % (step, time_rounded))\n",
    "\n",
    "        # File paths\n",
    "        image_files = []; image_positions=[]; cropping_regions=[]; image_scale_factors=[]\n",
    "\n",
    "        # viscosity slice\n",
    "        file_path_0 = os.path.join(prep_dir, \"%s_t%.4e.png\" % (file_name_list[0], time_rounded))\n",
    "        assert(os.path.isfile(file_path_0))\n",
    "        image_files.append(file_path_0)\n",
    "        image_positions.append((0, 100)) \n",
    "        cropping_regions.append(None)\n",
    "        image_scale_factors.append(1.5)\n",
    "\n",
    "        if need_framing:\n",
    "            # viscosity colorbar\n",
    "            file_path_0_c = \"/home/lochy/Documents/papers/documented_files/collision/color_viscosity_18_24-01.png\"\n",
    "            assert(os.path.isfile(file_path_0_c))\n",
    "            image_files.append(file_path_0_c)\n",
    "            image_positions.append((300, 650)) \n",
    "            cropping_regions.append(None)\n",
    "            image_scale_factors.append(1.5)\n",
    "\n",
    "        # temperature slice\n",
    "        file_path_0 = os.path.join(prep_dir, \"%s_t%.4e.png\" % (file_name_list[1], time_rounded))\n",
    "        assert(os.path.isfile(file_path_0))\n",
    "        image_files.append(file_path_0)\n",
    "        image_positions.append((2000, 100)) \n",
    "        cropping_regions.append(None)\n",
    "        image_scale_factors.append(1.5)\n",
    "        \n",
    "        if need_framing:\n",
    "            # temperature colorbar\n",
    "            file_path_2_c = \"/home/lochy/Documents/papers/documented_files/collision/color_temperature_0_2000.png\"\n",
    "            assert(os.path.isfile(file_path_2_c))\n",
    "            image_files.append(file_path_2_c)\n",
    "            image_positions.append((1900, 700)) \n",
    "            cropping_regions.append(None)\n",
    "            image_scale_factors.append(1.5)\n",
    "\n",
    "        # density slice\n",
    "        file_path_0 = os.path.join(prep_dir, \"%s_t%.4e.png\" % (file_name_list[2], time_rounded))\n",
    "        assert(os.path.isfile(file_path_0))\n",
    "        image_files.append(file_path_0)\n",
    "        image_positions.append((0, 1200)) \n",
    "        cropping_regions.append(None)\n",
    "        image_scale_factors.append(1.5)\n",
    "        \n",
    "        if need_framing:\n",
    "            # density colorbar\n",
    "            file_path_2_c = \"/home/lochy/Documents/papers/documented_files/collision/color_density_3000_4000-01.png\"\n",
    "            assert(os.path.isfile(file_path_2_c))\n",
    "            image_files.append(file_path_2_c)\n",
    "            image_positions.append((300, 1450)) \n",
    "            cropping_regions.append(None)\n",
    "            image_scale_factors.append(1.5)\n",
    "\n",
    "        # Combine images\n",
    "        output_image_file = os.path.join(prep_dir, \"%s_t%.4e.png\" % (animation_name, _time))\n",
    "        # Remove existing output image to ensure a clean overlay\n",
    "        if os.path.isfile(output_image_file):\n",
    "            os.remove(output_image_file)\n",
    "        # Call overlay function\n",
    "        plot_helper.overlay_images_on_blank_canvas(\n",
    "            canvas_size=(4200, 2500),  # Size of the blank canvas in pixels (width, height)\n",
    "            image_files=image_files,  # List of image file paths to overlay\n",
    "            image_positions=image_positions,  # Positions of each image on the canvas\n",
    "            cropping_regions=cropping_regions,  # Optional cropping regions for the images\n",
    "            image_scale_factors=image_scale_factors,  # Scaling factors for resizing the images\n",
    "            output_image_file=output_image_file  # Path to save the final combined image\n",
    "        )\n",
    "\n",
    "        # Add time stamp\n",
    "        text = \"t = %.1f Ma\" % (time_rounded / 1e6)  # Replace with the text you want to add\n",
    "        position = (25, 0)  # Replace with the desired text position (x, y)\n",
    "        font_path = \"/usr/share/fonts/truetype/msttcorefonts/times.ttf\"  # Path to Times New Roman font\n",
    "        font_size = 56\n",
    "\n",
    "        plot_helper.add_text_to_image(output_image_file, output_image_file, text, position, font_path, font_size)\n",
    "\n",
    "        ani_file_paths.append(output_image_file)\n",
    "\n",
    "    # Generate animation\n",
    "    if not debug_step0_animate_2d_case_basic:\n",
    "        ani_dir = os.path.join(local_dir_2d, \"img\", \"animation\")\n",
    "        if not os.path.isdir(ani_dir):\n",
    "            os.mkdir(ani_dir)\n",
    "        output_file = os.path.join(local_dir_2d, \"img\", \"animation\", \"%s.avi\" % animation_name)\n",
    "        create_avi_from_images(ani_file_paths, output_file, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmgeolib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
